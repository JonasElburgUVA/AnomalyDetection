{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "import pickle\n",
    "import preprocessing\n",
    "import utils\n",
    "import nets_LV\n",
    "\n",
    "from utils import mri_sample\n",
    "\n",
    "from sklearn import metrics\n",
    "from skimage.transform import resize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5.0, 5.0]\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "plt.rcParams['font.size'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 'small'\n",
    "plt.rcParams['figure.titlesize'] = 'medium'\n",
    "\n",
    "plt.rcParams['lines.linewidth'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the last 20 MRIs to the holdout folder as the\n",
    "# authors do (they do not specify which files, so we\n",
    "# assume the last 20)\n",
    "!mv ../data/brain_train_out/00780.nt ../data/brain_data/holdout/00780.nt\n",
    "!mv ../data/brain_train_out/00781.nt ../data/brain_data/holdout/00781.nt\n",
    "!mv ../data/brain_train_out/00782.nt ../data/brain_data/holdout/00782.nt\n",
    "!mv ../data/brain_train_out/00783.nt ../data/brain_data/holdout/00783.nt\n",
    "!mv ../data/brain_train_out/00784.nt ../data/brain_data/holdout/00784.nt\n",
    "!mv ../data/brain_train_out/00785.nt ../data/brain_data/holdout/00785.nt\n",
    "!mv ../data/brain_train_out/00786.nt ../data/brain_data/holdout/00786.nt\n",
    "!mv ../data/brain_train_out/00787.nt ../data/brain_data/holdout/00787.nt\n",
    "!mv ../data/brain_train_out/00788.nt ../data/brain_data/holdout/00788.nt\n",
    "!mv ../data/brain_train_out/00789.nt ../data/brain_data/holdout/00789.nt\n",
    "!mv ../data/brain_train_out/00790.nt ../data/brain_data/holdout/00790.nt\n",
    "!mv ../data/brain_train_out/00791.nt ../data/brain_data/holdout/00791.nt\n",
    "!mv ../data/brain_train_out/00792.nt ../data/brain_data/holdout/00792.nt\n",
    "!mv ../data/brain_train_out/00793.nt ../data/brain_data/holdout/00793.nt\n",
    "!mv ../data/brain_train_out/00794.nt ../data/brain_data/holdout/00794.nt\n",
    "!mv ../data/brain_train_out/00795.nt ../data/brain_data/holdout/00795.nt\n",
    "!mv ../data/brain_train_out/00796.nt ../data/brain_data/holdout/00796.nt\n",
    "!mv ../data/brain_train_out/00797.nt ../data/brain_data/holdout/00797.nt\n",
    "!mv ../data/brain_train_out/00798.nt ../data/brain_data/holdout/00798.nt\n",
    "!mv ../data/brain_train_out/00799.nt ../data/brain_data/holdout/00799.nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the other MRIs to the training folder\n",
    "!mv ../data/brain_train_out/*.nt ../data/brain_data/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../data/brain_data/train'\n",
    "holdout_dir = '../data/brain_data/holdout'\n",
    "\n",
    "train_dset = preprocessing.brain_dataset(data_dir=train_dir, sample_number = 8, train = True)\n",
    "train_loader = DataLoader(train_dset, batch_size=8,shuffle=True,num_workers=8,\n",
    "                         collate_fn = preprocessing.collate_fn)\n",
    "\n",
    "holdout_dset = preprocessing.brain_dataset(data_dir=holdout_dir, sample_number = 8, train = False)\n",
    "holdout_loader = DataLoader(holdout_dset, batch_size=8,shuffle=True,num_workers=8,\n",
    "                         collate_fn = preprocessing.collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve a sample (8 slices from 8 volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/vqvae/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/anaconda3/envs/vqvae/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/anaconda3/envs/vqvae/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/walter/Developer/education/dl2-project/AnomalyDetection-Fast/AnomalyDetection/OriginalPaper/preprocessing.py\", line 42, in __getitem__\n    sample = pickle.load(open(file_name, 'rb'))\nAttributeError: Can't get attribute 'mri_sample' on <module '__main__' (built-in)>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_hl \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_loader))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vqvae/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vqvae/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vqvae/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1360\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vqvae/lib/python3.9/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/vqvae/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/anaconda3/envs/vqvae/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/anaconda3/envs/vqvae/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/walter/Developer/education/dl2-project/AnomalyDetection-Fast/AnomalyDetection/OriginalPaper/preprocessing.py\", line 42, in __getitem__\n    sample = pickle.load(open(file_name, 'rb'))\nAttributeError: Can't get attribute 'mri_sample' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "X_hl = next(iter(holdout_loader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are in the attribute img of the mri_sample object. Coordinates are in coord (used to condition both VQ-VAE and AR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_hl.img.shape)\n",
    "print(X_hl.coord.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_hl.img[42])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "## VQ-VAE\n",
    "\n",
    "Training of VQ-VAE network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nets_LV.VQVAE(1,n_channels=(16,32,64,256),code_size=128,n_res_block=2,cond_channels=1,dropout_p=.1)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "tracker = utils.train_tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.train_epochs(model, optimizer, tracker, train_loader, holdout_loader, epochs=600, device = device,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.plot(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility below allows to save / load checkpoints of the training. Note that when training Prior AR below, VQ-VAE model parameters will be loaded from a checkpint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_checkpoint(model,optimizer,tracker,'./checkpoints/brain_vqvae.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chpt = torch.load('./checkpoints/brain_vqvae.pt')\n",
    "model.load_state_dict(chpt['model'])\n",
    "optimizer.load_state_dict(chpt['optimizer'])\n",
    "tracker = chpt['tracker']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior AR\n",
    "Definition and training of prior model. feat_ext_mdl is the vq-vae network that must have been trained in section above, its weights are loaded from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_ext_mdl = nets_LV.VQVAE(1,n_channels=(16,32,64,256),code_size=128,n_res_block=2,cond_channels=1, dropout_p=.1)\n",
    "\n",
    "feat_ext_mdl.to(device)\n",
    "chpt = torch.load('./checkpoints/brain_vqvae.pt')\n",
    "feat_ext_mdl.load_state_dict(chpt['model'])\n",
    "\n",
    "model = nets_LV.VQLatentSNAIL(feature_extractor_model=feat_ext_mdl,\n",
    "                              shape=(20,20), n_block = 4, n_res_block = 4, n_channels = 128, cond_channels=1)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "tracker = utils.train_tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.train_epochs(model, optimizer, tracker, train_loader, holdout_loader, epochs=600, device = device,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.plot(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_checkpoint(model,optimizer,tracker,'./checkpoints/brain_prior.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chpt = torch.load('./checkpoints/brain_prior.pt')\n",
    "model.load_state_dict(chpt['model'])\n",
    "optimizer.load_state_dict(chpt['optimizer'])\n",
    "tracker = chpt['tracker']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection\n",
    "## Loading of toy examples\n",
    "\n",
    "Only 32 slices for each volume are processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_volume_brain(source_file, slices = 32):\n",
    "        nimg = nib.load(source_file)\n",
    "        nimg_array = nimg.get_fdata()\n",
    "        vol_s = nimg_array.shape\n",
    "\n",
    "        nimg_array = resize(nimg_array, (160, 160, slices))\n",
    "        nimg_array = nimg_array.transpose((2,1,0))\n",
    "        nimg_array = nimg_array[:, ::-1, :]\n",
    "        \n",
    "        # Normalize non-zeros\n",
    "        nimg_array[nimg_array<0.05] = 0\n",
    "\n",
    "        non_zero_mask = np.where(nimg_array>0.05)\n",
    "        mu,sigma = nimg_array[non_zero_mask].mean(),nimg_array[non_zero_mask].std()\n",
    "        nimg_array[non_zero_mask] = (nimg_array[non_zero_mask] - mu) / (sigma+1e-5)\n",
    "        \n",
    "        coord = np.linspace(-.5,.5,slices)[:, np.newaxis]\n",
    "        \n",
    "        # FIXME: This may require some modifications\n",
    "        img_batch = mri_sample(nimg_array, coord, None)\n",
    "        \n",
    "        return img_batch, vol_s, nimg.affine\n",
    "\n",
    "def load_seg_brain(source_file, slices = 32):\n",
    "        nimg = nib.load(source_file)\n",
    "        nimg_array = nimg.get_fdata()\n",
    "        vol_s = nimg_array.shape\n",
    "\n",
    "        nimg_array = resize(nimg_array, (160, 160, slices))\n",
    "        nimg_array = nimg_array.transpose((2,1,0))\n",
    "        nimg_array = nimg_array[:, ::-1, :]\n",
    "        nimg_array = (nimg_array > .5)\n",
    "        return nimg_array\n",
    "    \n",
    "toy_dir = '/home/sergio/Documents/bigdrive/data/mood/data_toy/brain/toy'\n",
    "toy_seg_dir = '/home/sergio/Documents/bigdrive/data/mood/data_toy/brain/toy_label/pixel'\n",
    "\n",
    "mood_test = []\n",
    "mood_test_seg = []\n",
    "\n",
    "for i in range(4):\n",
    "    mood_test.append(load_volume_brain(os.path.join(toy_dir,'toy_{}.nii.gz'.format(i)))[0])\n",
    "    mood_test_seg.append(load_seg_brain(os.path.join(toy_seg_dir,'toy_{}.nii.gz'.format(i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of image / segmentation (32,160,160)\n",
    "print(mood_test[0].img.shape)\n",
    "print(mood_test_seg[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample-wise score\n",
    "\n",
    "\n",
    "Sample-wise score is derived from the density of the latent variables as estimated by the AR prior model. Specifically, we hypothesize that the prior model will assign low probability to the latent variables encoding image areas where anomalies exist. \n",
    "\n",
    "We use as score the estimated cross-entropy for the latent variables, considering only the values for the latent variables with cross-entropy above a threshold λ. The 98 percentile of the cross-entropy values observed in the holdout dataset has been used as threshold, in both datasets this corresponded broadly to the value 7. \n",
    "\n",
    "$$Score_{sample} = \\sum_{i=1}^N \\xi(p(x))$$\n",
    "\n",
    "$$\n",
    "\\xi(z) = \\begin{cases}\n",
    "     -\\log(z) \\text{ if} -\\log(z) > \\lambda \\\\\n",
    "     0 \\text{ otherwise} \\\\\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "\n",
    "model.eval()\n",
    "for X in mood_test:\n",
    "    x_img = torch.from_numpy(X.img.copy()).to(device).float()\n",
    "    x_coord = torch.from_numpy(X.coord).to(device).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        codes = model.retrieve_codes(x_img,x_coord).flatten(1)\n",
    "        loss = model.loss(x_img,cond = x_coord, reduction='none')['loss'].flatten(1)\n",
    "        \n",
    "        score = torch.sum(loss*(loss>7),1).float()\n",
    "        pred.append([score.cpu().numpy()])\n",
    "\n",
    "pred = np.concatenate(pred,1).reshape(4,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a slice no [0,32] for each of the volumes with the scores for the slice\n",
    "no = 10\n",
    "fig,ax = plt.subplots(1,4,figsize = (8,8))\n",
    "\n",
    "[a.imshow(mood_test[i].img[no],vmin=-3,vmax=3) for i,a in enumerate(ax)]\n",
    "[a.set_title('Score {:.1f}'.format(pred[i,no])) for i,a in enumerate(ax)]\n",
    "[a.axis('off') for a in ax]\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the docker submission, slice scores are normalized dividing by 200 and clipping to 1.\n",
    "\n",
    "The score for the volume is just the sum of the slices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel-wise scores\n",
    "\n",
    "Pixel-wise score also utilizes densities estimated by the AR model. We restore anomalous slices by replacing high-loss latent codes with low-loss codes sampled by the AR model and decoding to pixel space using the VQ-VAE decoder. \n",
    "\n",
    "This process generates restorations of the original image, where encoded areas with low probability are replaced with normal areas as observed in the training set. For both datasets our method resamples latent variables with cross-entropy greater than 5, this correspond to the percentile 90 in holdout dataset.\n",
    "\n",
    "Pixel-wise anomaly score is obtained comparing in pixel space the original image with the restorations, specifically:\n",
    "\n",
    "$$Score_{pixel} = \\sum_{j=1}^S \\varphi_j\\|y-r_j\\|_{1}$$\n",
    "\n",
    "Where $y$ is the test image pixel intensity, $r_j$ the pixel intensity for the restoration $j$ ∈ 1,2,...,S. We noted that the increase in samples contributes to reduce the variance of anomaly predictors, 15 samples are used in the submission.\n",
    "\n",
    "Given the sequential sampling by the AR model we observed that some of the samples drift too much from the original image and contribute negatively to the anomaly localization. In order to address this issue, we introduced the weighting factor $\\varphi$ defined as: \n",
    "\n",
    "\n",
    "$$\\varphi_j = \\text{softmax}(1/\\sum_{i=1}^D\\|y^i-r^i_j\\|_{1})$$\n",
    "\n",
    "Where the sum is over D pixels in the original pixel space. Note that this factor weights restorations based on the inverse of the distance between restorations and image, removing weight from restorations that are too different from the original image. Also note that modifying the softmax temperature allows the pixel score to transition from the mean of residuals among restorations to the residuals of the nearest restoration. In submission we use 3. as temperature because it seems to work best in the toy examples. Note that the introduction of the weight has a higher impact in the Abdominal dataset where restorations have a higher variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def restore(model, n, img, cond = None, threshold_log_p = 5):\n",
    "    \"\"\" Generate n restorations of images\n",
    "        Returns restorations, both images and codes \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    #Use VQ-VAE to encode original image\n",
    "    codes = model.retrieve_codes(img,cond)\n",
    "    code_size = codes.shape[-2:]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        samples = codes.clone().unsqueeze(1).repeat(1,n,1,1).reshape(img.shape[0]*n,*code_size)\n",
    "        cond_repeat = cond.unsqueeze(1).repeat(1,n,1).reshape(img.shape[0]*n,-1)\n",
    "        \n",
    "        # Iterate through latent variables. \n",
    "        for r in range(code_size[0]):\n",
    "            for c in range(code_size[1]):\n",
    "\n",
    "                logits = model.forward_latent(samples, cond_repeat)[:, :, r, c]\n",
    "                loss = F.cross_entropy(logits, samples[:, r, c], reduction='none')\n",
    "\n",
    "                # Replace sample if above threshold\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                samples[loss > threshold_log_p, r, c] = torch.multinomial(probs, 1).squeeze(-1)[loss > threshold_log_p]\n",
    "        \n",
    "        # Retrieve z for the latent codes\n",
    "        z = model.feature_extractor_model.codebook.embedding(samples.unsqueeze(1))\n",
    "        z = z.squeeze(1).permute(0,3,1,2).contiguous()\n",
    "        \n",
    "        # Decode to pixel space splitting computation in batches\n",
    "        x_tilde = []\n",
    "        for i in range(img.shape[0]):\n",
    "            x_tilde.append(model.feature_extractor_model.decode(z[i*n:(i+1)*n],\n",
    "                                               cond_repeat[i*n:(i+1)*n]))\n",
    "        x_tilde = torch.cat(x_tilde)\n",
    "              \n",
    "    return x_tilde.reshape(img.shape[0],n,*img.shape[-2:]), samples.reshape(img.shape[0],n,*code_size)\n",
    "\n",
    "mood_test_reconstructions = []\n",
    "for X in mood_test:\n",
    "\n",
    "    img = torch.from_numpy(X.img.copy()).to(device).float()\n",
    "    coord = torch.from_numpy(X.coord.copy()).to(device).float()\n",
    "\n",
    "    x_tilde, latent_sample = restore(model,n=15,img=img, cond=coord, threshold_log_p=5)\n",
    "    mood_test_reconstructions.append(x_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.from_numpy(np.concatenate([x.img for x in mood_test]))\n",
    "img_tilde = torch.cat(mood_test_reconstructions)\n",
    "img_seg = torch.from_numpy(np.concatenate(mood_test_seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4 volumes x 32 slices, 15 restorations, 160,160)\n",
    "print(img_tilde.shape)\n",
    "\n",
    "# (4 volumes x 32 slices,160,160)\n",
    "print(img_seg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually compare original image with restoration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_no = 1\n",
    "slice_no = 9\n",
    "\n",
    "fig, ax = plt.subplots(2,6, figsize=(8,2.5))\n",
    "ax[0,0].imshow(mood_test[img_no].img[slice_no])\n",
    "ax[0,0].axis('off')\n",
    "ax[1,0].imshow(mood_test_seg[img_no][slice_no])\n",
    "ax[1,0].axis('off')\n",
    "\n",
    "for i in range(1,6):\n",
    "    ax[0,i].imshow(mood_test_reconstructions[img_no][slice_no,i-1].cpu())\n",
    "    ax[0,i].axis('off')\n",
    "\n",
    "for i in range(1,6):\n",
    "    ax[1,i].imshow(mood_test_reconstructions[img_no][slice_no,i-1+5].cpu())\n",
    "    ax[1,i].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1(original image, restoration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,6, figsize=(8,4))\n",
    "ax[0,0].imshow(mood_test[img_no].img[slice_no])\n",
    "ax[0,0].axis('off')\n",
    "ax[1,0].imshow(mood_test_seg[img_no][slice_no])\n",
    "ax[1,0].axis('off')\n",
    "for i in range(1,6):\n",
    "    tmp = np.abs(mood_test_reconstructions[img_no][slice_no,i-1].cpu() - mood_test[img_no].img[slice_no])\n",
    "    ax[0,i].imshow(tmp,vmax=2)\n",
    "    ax[0,i].axis('off')\n",
    "for i in range(1,6):\n",
    "    tmp = np.abs(mood_test_reconstructions[img_no][slice_no,i-1+5].cpu()- mood_test[img_no].img[slice_no])\n",
    "    ax[1,i].imshow(tmp,vmax=2)    \n",
    "    ax[1,i].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing of predictions\n",
    "# Restorations are not as sharp as the original images which causes edges to have high variance. MinPooling is \n",
    "# used to smooth the edges (-MaxPooling(-x))\n",
    "\n",
    "smooth = nn.Sequential(nn.MaxPool3d(kernel_size=3,padding=1,stride=1),\n",
    "                       nn.AvgPool3d(kernel_size=(3,7,7),padding=(1,3,3),stride=1),\n",
    "                      ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 distance restoration vs original image\n",
    "diffs = torch.abs(img.unsqueeze(1).to(device).float()-img_tilde)\n",
    "\n",
    "# Calculate restoration weight based on restoration similarity to original image\n",
    "sim_imgwise = torch.mean(diffs,(2,3)).unsqueeze(2).unsqueeze(3)\n",
    "sim_imgwise = torch.softmax(3/sim_imgwise,1)\n",
    "\n",
    "# Pixel-wise score after weight \n",
    "diffs = (diffs*sim_imgwise).sum(1,keepdims=True)\n",
    "\n",
    "# Post-process using Min Pool and average filter\n",
    "diffs = diffs.squeeze().unsqueeze(0).unsqueeze(0)\n",
    "smooth_diffs = -smooth(-diffs)\n",
    "smooth_diffs = smooth_diffs.squeeze().unsqueeze(1)\n",
    "\n",
    "# Average precission\n",
    "metrics.average_precision_score(img_seg.flatten(),smooth_diffs.flatten().clamp_max(1.).cpu() / 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROCAUC\n",
    "fpr, tpr, thresholds = metrics.roc_curve(img_seg.flatten(), smooth_diffs.flatten().cpu())\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice Score\n",
    "utils.max_score(img_seg.flatten(),smooth_diffs.flatten().cpu(), steps = 20)[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4, figsize=(6,4))\n",
    "ax[0].imshow(mood_test[img_no].img[slice_no])\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(mood_test_seg[img_no][slice_no])\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Segmentation')\n",
    "ax[2].imshow(diffs.squeeze()[img_no * 32 + slice_no].cpu())\n",
    "ax[2].axis('off')\n",
    "ax[2].set_title('Score')\n",
    "ax[3].imshow(smooth_diffs[img_no * 32 + slice_no].squeeze().cpu())\n",
    "ax[3].axis('off')\n",
    "ax[3].set_title('Smoothed score')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly score histogram, anomalous vs normal\n",
    "plt.hist(smooth_diffs.flatten().cpu()[img_seg.flatten()==0],alpha=.3,bins=20,density=True)\n",
    "plt.hist(smooth_diffs.flatten().cpu()[img_seg.flatten()==1],alpha=.3,bins=20,density=True)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
