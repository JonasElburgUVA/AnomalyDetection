{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import utils\n",
    "import nets_LV\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ColorJitter(brightness=.3, hue=.2),\n",
    "    # transforms.RandomRotation(degrees=(-10, 10)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_set = '../../../../../project/gpuuva022/shared/AnomalyDetection/FFHQ_Data/FFHQ_data/l_tuning'\n",
    "images_pixel_score_path = '../../data/thresholding_images/'#'/home/lcur1737/AnomalyDetection/data/thresholding_images'\n",
    "\n",
    "# val_dataset = ImageFolder(lambda_set, transform=transform_pipeline)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "pixel_score_set = ImageFolder(images_pixel_score_path, transform=transform_pipeline)\n",
    "pixel_score_loader = DataLoader(pixel_score_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_checkpoint_path = '../../checkpoints/ffhq_continued_020.pt' #'/home/lcur1737/AnomalyDetection/src/checkpoints/VQVAE/ffhq_continued_020.pt'\n",
    "ar_checkpoint_path = '../../checkpoints/ffhq_ar_030.pt' #'/home/lcur1737/AnomalyDetection/src/checkpoints/AR/ffhq_ar_030.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "vq_model = nets_LV.VQVAE(\n",
    "    d=3,\n",
    "    n_channels=(16, 32, 64, 256),\n",
    "    code_size=128,\n",
    "    n_res_block=2,\n",
    "    dropout_p=.1\n",
    ").to(device)\n",
    "\n",
    "vqvae_checkpoint = torch.load(vqvae_checkpoint_path, map_location=device)\n",
    "vq_model.load_state_dict(vqvae_checkpoint[\"model\"])\n",
    "# vq_model = vq_model.to(device)\n",
    "\n",
    "ar_model = nets_LV.VQLatentSNAIL(\n",
    "    feature_extractor_model=vq_model,\n",
    "    shape=(16, 16),\n",
    "    n_block=4,\n",
    "    n_res_block=4,\n",
    "    n_channels=128\n",
    ").to(device)\n",
    "\n",
    "ar_checkpoint = torch.load(ar_checkpoint_path, map_location=device)\n",
    "\n",
    "ar_model.load_state_dict(ar_checkpoint['model'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample wise score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "accuracy = []\n",
    "ar_model.eval()\n",
    "\n",
    "thresholds = np.linspace(1,20,50)\n",
    "\n",
    "\n",
    "# loaded_dataloader = tqdm(val_dataloader)\n",
    "# print('Starting the measurements')\n",
    "# for thr in thresholds:\n",
    "#     for batchX, batchY in loaded_dataloader:\n",
    "#         with torch.no_grad():\n",
    "#             # codes = ar_model.retrieve_codes(batchX).flatten(1)\n",
    "#             loss = ar_model.loss(batchX, reduction='none')['loss'].flatten(1)\n",
    "            \n",
    "#             score = torch.sum(loss*(loss>thr), 1).float()\n",
    "#             pred.append([score.cpu().numpy()])\n",
    "#         break\n",
    "#     break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find pixel score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(n, img, threshold_log_p = 5):\n",
    "    \"\"\" Generates n reconstructions for each image in img.\n",
    "    Resamples latent variables with cross-entropy > threshold\n",
    "    Returns corrected images and associated latent variables\"\"\"\n",
    "          \n",
    "    #Use VQ-VAE to encode original image\n",
    "    codes = ar_model.retrieve_codes(img)\n",
    "    code_size = codes.shape[-2:]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        samples = codes.clone().unsqueeze(1).repeat(1,n,1,1).reshape(img.shape[0]*n,*code_size)\n",
    "        logits = ar_model.forward_latent(samples)\n",
    "\n",
    "        for r in range(code_size[0]):\n",
    "            for c in range(code_size[1]):\n",
    "                code_logits = logits[:, :, r, c]\n",
    "\n",
    "                loss = F.cross_entropy(code_logits, samples[:, r, c], reduction='none')\n",
    "                probs = F.softmax(code_logits, dim=1)\n",
    "\n",
    "                samples[loss > threshold_log_p, r, c] = torch.multinomial(probs, 1).squeeze(-1)[loss > threshold_log_p]\n",
    "                # print(sum(loss > threshold_log_p))\n",
    "\n",
    "        z = vq_model.codebook.embedding(samples.unsqueeze(1))\n",
    "        z = z.squeeze(1).permute(0,3,1,2).contiguous()\n",
    "        \n",
    "        # Split the calculation in batches\n",
    "        x_tilde = []\n",
    "        for i in range(img.shape[0]):\n",
    "            x_tilde.append(vq_model.decode(z[i*n:(i+1)*n]))\n",
    "        x_tilde = torch.cat(x_tilde)\n",
    "        \n",
    "        \n",
    "    return x_tilde.reshape(3,n,*img.shape[-2:]), samples.reshape(img.shape[0],n,*code_size)\n",
    "\n",
    "reconstructions = []\n",
    "\n",
    "# for X,_ in loaded_dataloader:\n",
    "#     x_tilde, latent_sample = reconstruct(n=15,img=X, threshold_log_p=5)\n",
    "#     reconstructions.append(x_tilde)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 images experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(iter(pixel_score_loader))[0]\n",
    "\n",
    "\n",
    "reconstruction500 = torch.mean(reconstruct(10,X, threshold_log_p=500)[0], dim=1)\n",
    "reconstruction20 = torch.mean(reconstruct(10,X, threshold_log_p=20)[0], dim=1)\n",
    "reconstruction10 = torch.mean(reconstruct(10,X, threshold_log_p=10)[0], dim=1)\n",
    "reconstruction5 = reconstruct(1,X, threshold_log_p=5)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,5 , figsize=(20, 10))\n",
    "# show images\n",
    "axes[0].imshow(X[0].permute(1,2,0))\n",
    "axes[1].imshow(reconstruction500.squeeze().permute(1,2,0))\n",
    "axes[2].imshow(reconstruction20.squeeze().permute(1,2,0))\n",
    "axes[3].imshow(reconstruction10.squeeze().permute(1,2,0))\n",
    "axes[4].imshow(reconstruction5.squeeze().permute(1,2,0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
