{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding\n",
    "This notebook will guide you through finding the optimal threshold hyperparameters for your data and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcur1737/.conda/envs/vae/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmartentyrk\u001b[0m (\u001b[33mvqvaeanomaly\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from ..utils import *\n",
    "import utils\n",
    "import nets_LV\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from sklearn.metrics import precision_score, accuracy_score, average_precision_score, roc_auc_score\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your paths here. Make sure your data is structured as follows:\n",
    "\n",
    "- data\n",
    "    - FaceForensics\n",
    "        - test_set\n",
    "            - fake\n",
    "                - 000_003\n",
    "                    - face_0_0_10.jpg\n",
    "                    - ...\n",
    "                - ...\n",
    "            - real\n",
    "                - 000\n",
    "                - ...\n",
    "         - validation_set\n",
    "            - ...\n",
    "    - FFHQ\n",
    "        - test_set\n",
    "            - easy\n",
    "                - 1 # indicating fake\n",
    "                    - 1_1110.png\n",
    "                    - ...\n",
    "                - 0 # indicating real\n",
    "                    - 51010.png\n",
    "                    - ...\n",
    "            - medium\n",
    "                - ...\n",
    "            - hard\n",
    "                - ...\n",
    "        - validation_set\n",
    "            - ...\n",
    "    - Output\n",
    "        - faceforensics\n",
    "        - ffhq\n",
    "    - Checkpoints\n",
    "        - ffhq_vqvae.pt\n",
    "        - ffhq_ar.pt\n",
    "        - faceforensics_vqvae.pt\n",
    "        - faceforensics_ar.pt\n",
    "\n",
    "and the checkpoints are named {dataset}_{model}.pt for dataset in ['ffhq', 'faceforensics'] and model in ['vqvae', 'ar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE DATASET HERE AND SET PATHS AS DESCRIBED, THEN NONE OF THE CODE BELOW NEEDS TO BE CHANGED\n",
    "data_dir = '../../data'\n",
    "checkpoint_path = 'checkpoints'\n",
    "dataset = 'faceforensics' # choose from ['ffhq', 'faceforensics']\n",
    "# mode = 'sample' # choose from ['sample', 'pixel']\n",
    "split = 'test' # choose from ['val', 'test']\n",
    "pruned = False # set to True if you want to use the smaller faceforensics dataset, e.g. for hyperparameter tuning\n",
    "thresholds = [7] # Only input 1 if testing\n",
    "difficulties = ['easy', 'medium', 'hard'] # only used for ffhq\n",
    "assert dataset in ['ffhq', 'faceforensics']\n",
    "txt_to_label = {'real': 0, 'fake': 1}\n",
    "\n",
    "if pruned:\n",
    "  pr=\"_pruned\"\n",
    "else:\n",
    "  pr=\"\"\n",
    "\n",
    "if dataset == \"ffhq\":\n",
    "  ffhq_sets = {}\n",
    "  ffhq_loaders = {}\n",
    "  for dif in difficulties:\n",
    "    ffhq_sets[dif]= ImageFolder(os.path.join(data_dir, dataset, split, dif), transform=transform)\n",
    "    ffhq_loaders[dif] = DataLoader(ffhq_sets[dif], batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "faceforensics data is loaded without imagefolder, see the prediction loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_cp = torch.load(os.path.join(checkpoint_path, dataset + '_vqvae.pt'), map_location=device)\n",
    "ar_cp = torch.load(os.path.join(checkpoint_path, dataset + '_ar.pt'), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "vq_model = nets_LV.VQVAE(\n",
    "    d=3,\n",
    "    n_channels=(16, 32, 64, 256),\n",
    "    code_size=128,\n",
    "    n_res_block=2,\n",
    "    dropout_p=.1\n",
    ").to(device)\n",
    "\n",
    "vq_model.load_state_dict(vqvae_cp[\"model\"])\n",
    "vq_model = vq_model.to(device)\n",
    "\n",
    "ar_model = nets_LV.VQLatentSNAIL(\n",
    "    feature_extractor_model=vq_model,\n",
    "    shape=(16, 16),\n",
    "    n_block=4,\n",
    "    n_res_block=4,\n",
    "    n_channels=128\n",
    ").to(device)\n",
    "\n",
    "\n",
    "ar_model.load_state_dict(ar_cp['model'])\n",
    "ar_model = ar_model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate anomaly scores for various thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!find ../../data -name \".DS_Store\" -type f -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating {dataset} with thresholds {thresholds}\")\n",
    "input_dir = data_dir + f'/{dataset}'\n",
    "output_dir = data_dir + '/output'\n",
    "\n",
    "# bruh moment right here\n",
    "predictions = defaultdict(lambda: defaultdict())\n",
    "\n",
    "\n",
    "if dataset == \"ffhq\":\n",
    "    for thr in thresholds:\n",
    "        predictions[thr] = {}\n",
    "        with torch.no_grad():\n",
    "          for dif in ['easy', 'hard']:\n",
    "            predictions[thr][dif] = {'scores':[],'labels':[]}\n",
    "            for batch, cl in tqdm(ffhq_loaders[dif]):\n",
    "              loss = ar_model.loss(batch.to(device), reduction=\"none\")[\"loss\"].flatten(1)\n",
    "              scores = torch.sum(loss * (loss > thr), 1).float()\n",
    "              predictions[thr][dif]['scores'].append(scores.detach().cpu().numpy().tolist())\n",
    "              predictions[thr][dif]['labels'].append(cl.detach().cpu().numpy().tolist())\n",
    "            assert len(predictions[thr][dif]['scores']) == len(predictions[thr][dif]['labels'])\n",
    "    with open(os.path.join(output_dir, \"ffhq\", \"scores.json\"), \"w\") as write_file:\n",
    "        json.dump(predictions, write_file)\n",
    "\n",
    "\n",
    "# mean version\n",
    "elif dataset == \"faceforensics\":\n",
    "   input_dir = os.path.join(data_dir, dataset, f'{split}_set_pruned')\n",
    "   for thr in thresholds:\n",
    "      predictions[thr] = {'scores':[],'labels':[]}\n",
    "      with torch.no_grad():\n",
    "          for rf in os.listdir(input_dir):\n",
    "            for img_id in tqdm(os.listdir(os.path.join(input_dir, rf))):\n",
    "              imgdir = os.listdir(os.path.join(input_dir, rf, img_id)) \n",
    "              batch = torch.stack([transform(Image.open(os.path.join(input_dir, rf, img_id, img))) for img in imgdir]).to(device)\n",
    "              loss = ar_model.loss(batch, reduction=\"none\")[\"loss\"].flatten(1)\n",
    "              scores = torch.sum(loss * (loss > thr), 1).float()\n",
    "              score = scores.mean()\n",
    "              predictions[thr]['scores'].append(score.detach().cpu().numpy().tolist())\n",
    "              predictions[thr]['labels'].append(txt_to_label[rf])\n",
    "\n",
    "with open(os.path.join(output_dir, \"faceforensics\",\"scores.json\"), \"w\") as write_file:\n",
    "        json.dump(predictions, write_file)\n",
    "               \n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample wise score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir + f\"/{dataset}/scores.json\") as s:\n",
    "    scores = json.load(s)\n",
    "\n",
    "\n",
    "if dataset == 'ffhq':\n",
    "  res = defaultdict(lambda: defaultdict(lambda: {'auroc': None, 'ap': None}))\n",
    "  for thr in thresholds:\n",
    "    thr = str(thr)\n",
    "    for dif in ['easy', 'hard']:\n",
    "      labels = np.concatenate(scores[thr][dif]['labels'])\n",
    "      preds = np.concatenate(scores[thr][dif]['scores'])\n",
    "\n",
    "      auroc = roc_auc_score(labels,preds)\n",
    "      ap = average_precision_score(labels,preds, pos_label=1)\n",
    "\n",
    "      res[thr][dif]['auroc'] = auroc\n",
    "      res[thr][dif]['ap'] = ap\n",
    "\n",
    "elif dataset == 'faceforensics':\n",
    "  res = defaultdict(lambda: {'auroc' : None, 'ap': None})\n",
    "  for thr in thresholds:\n",
    "    thr = str(thr)\n",
    "    labels = scores[thr]['labels']\n",
    "    preds = scores[thr]['scores']\n",
    "\n",
    "    auroc = roc_auc_score(labels,preds)\n",
    "    ap = average_precision_score(labels,preds, pos_label=1)\n",
    "\n",
    "    res[thr]['auroc'] = auroc\n",
    "    res[thr]['ap'] = ap\n",
    "\n",
    "for thr in thresholds:\n",
    "  print(res[str(thr)].items())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to save the best threshold!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find pixel score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(n, img, threshold_log_p = 5):\n",
    "    \"\"\" Generates n reconstructions for each image in img.\n",
    "    Resamples latent variables with cross-entropy > threshold\n",
    "    Returns corrected images and associated latent variables\"\"\"\n",
    "          \n",
    "    #Use VQ-VAE to encode original image\n",
    "    codes = ar_model.retrieve_codes(img)\n",
    "    code_size = codes.shape[-2:]\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        samples = codes.clone().unsqueeze(1).repeat(1,n,1,1).reshape(img.shape[0]*n,*code_size)\n",
    "\n",
    "        if not threshold_log_p == None:\n",
    "            for r in tqdm(range(code_size[0])):\n",
    "                for c in range(code_size[1]):        \n",
    "\n",
    "                    code_logits = ar_model.forward_latent(samples)[:,:,r,c]\n",
    "                    loss = F.cross_entropy(code_logits, samples[:, r, c], reduction='none')\n",
    "                    probs = F.softmax(code_logits, dim=1)\n",
    "\n",
    "                    samples[loss > threshold_log_p, r, c] = torch.multinomial(probs, 1).squeeze(-1)[loss > threshold_log_p]\n",
    "\n",
    "        z = vq_model.codebook.embedding(samples.unsqueeze(1))\n",
    "        z = z.squeeze(1).permute(0,3,1,2).contiguous()\n",
    "        \n",
    "        # Split the calculation in batches\n",
    "        x_tilde = []\n",
    "        for i in range(img.shape[0]):\n",
    "            x_tilde.append(vq_model.decode(z[i*n:(i+1)*n]))\n",
    "        x_tilde = torch.cat(x_tilde)\n",
    "        \n",
    "        \n",
    "    return x_tilde.reshape(img.shape[0]*img.shape[1],n,*img.shape[-2:]), samples.reshape(img.shape[0],n,*code_size)\n",
    "\n",
    "reconstructions = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
