{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcur1737/.conda/envs/vae/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmartentyrk\u001b[0m (\u001b[33mvqvaeanomaly\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import utils\n",
    "import nets_LV\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ColorJitter(brightness=.3, hue=.2),\n",
    "    # transforms.RandomRotation(degrees=(-10, 10)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_set = '../../../../../project/gpuuva022/shared/AnomalyDetection/FFHQ_Data/FFHQ_data/l_tuning'\n",
    "\n",
    "images_pixel_score_path = '/home/lcur1737/AnomalyDetection/data/thresholding_images'\n",
    "val_dataset = ImageFolder(lambda_set, transform=transform_pipeline)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_checkpoint_path = '/home/lcur1737/AnomalyDetection/src/checkpoints/VQVAE/ffhq_continued_020.pt'\n",
    "ar_checkpoint_path = '/home/lcur1737/AnomalyDetection/src/checkpoints/AR/ffhq_ar_030.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vq_model = nets_LV.VQVAE(\n",
    "    d=3,\n",
    "    n_channels=(16, 32, 64, 256),\n",
    "    code_size=128,\n",
    "    n_res_block=2,\n",
    "    dropout_p=.1\n",
    ")\n",
    "\n",
    "vqvae_checkpoint = torch.load(vqvae_checkpoint_path)\n",
    "vq_model.load_state_dict(vqvae_checkpoint[\"model\"])\n",
    "# vq_model = vq_model.to(device)\n",
    "\n",
    "ar_model = nets_LV.VQLatentSNAIL(\n",
    "    feature_extractor_model=vq_model,\n",
    "    shape=(16, 16),\n",
    "    n_block=4,\n",
    "    n_res_block=4,\n",
    "    n_channels=128\n",
    ")\n",
    "\n",
    "ar_checkpoint = torch.load(ar_checkpoint_path)\n",
    "\n",
    "ar_model.load_state_dict(ar_checkpoint['model'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample wise score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "accuracy = []\n",
    "ar_model.eval()\n",
    "\n",
    "thresholds = np.linspace(1,20,50)\n",
    "\n",
    "\n",
    "loaded_dataloader = tqdm(val_dataloader)\n",
    "print('Starting the measurements')\n",
    "for thr in thresholds:\n",
    "    for batchX, batchY in loaded_dataloader:\n",
    "        with torch.no_grad():\n",
    "            # codes = ar_model.retrieve_codes(batchX).flatten(1)\n",
    "            loss = ar_model.loss(batchX, reduction='none')['loss'].flatten(1)\n",
    "            \n",
    "            score = torch.sum(loss*(loss>thr), 1).float()\n",
    "            pred.append([score.cpu().numpy()])\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find pixel score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(n, img, threshold_log_p = 5):\n",
    "    \"\"\" Generates n reconstructions for each image in img.\n",
    "    Resamples latent variables with cross-entropy > threshold\n",
    "    Returns corrected images and associated latent variables\"\"\"\n",
    "          \n",
    "    #Use VQ-VAE to encode original image\n",
    "    codes = ar_model.retrieve_codes(img)\n",
    "    code_size = codes.shape[-2:]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        samples = codes.clone().unsqueeze(1).repeat(1,n,1,1).reshape(img.shape[0]*n,*code_size)\n",
    "        logits = ar_model.forward_latent(samples)\n",
    "\n",
    "        for r in range(code_size[0]):\n",
    "            for c in range(code_size[1]):\n",
    "                code_logits = logits[:, :, r, c]\n",
    "\n",
    "                loss = F.cross_entropy(code_logits, samples[:, r, c], reduction='none')\n",
    "                probs = F.softmax(code_logits, dim=1)\n",
    "\n",
    "                samples[loss > threshold_log_p, r, c] = torch.multinomial(probs, 1).squeeze(-1)[loss > threshold_log_p]\n",
    "\n",
    "        z = vq_model.codebook.embedding(samples.unsqueeze(1))\n",
    "        z = z.squeeze(1).permute(0,3,1,2).contiguous()\n",
    "        \n",
    "        # Split the calculation in batches\n",
    "        x_tilde = []\n",
    "        for i in range(img.shape[0]):\n",
    "            x_tilde.append(vq_model.decode(z[i*n:(i+1)*n]))\n",
    "        x_tilde = torch.cat(x_tilde)\n",
    "        \n",
    "        \n",
    "    return x_tilde.reshape(img.shape[0],n,*img.shape[-2:]), samples.reshape(img.shape[0],n,*code_size)\n",
    "\n",
    "reconstructions = []\n",
    "\n",
    "for X,_ in loaded_dataloader:\n",
    "    x_tilde, latent_sample = reconstruct(n=15,img=X, threshold_log_p=5)\n",
    "    reconstructions.append(x_tilde)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 images experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
