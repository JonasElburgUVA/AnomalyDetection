{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcur1737/.conda/envs/vae/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmartentyrk\u001b[0m (\u001b[33mvqvaeanomaly\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nets_LV\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from sklearn.metrics import precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ColorJitter(brightness=.3, hue=.2),\n",
    "    # transforms.RandomRotation(degrees=(-10, 10)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_set = '../../../../../project/gpuuva022/shared/AnomalyDetection/FFHQ_Data/FFHQ_data/l_tuning'\n",
    "images_pixel_score_path = '../../data/thresholding_images/'#'/home/lcur1737/AnomalyDetection/data/thresholding_images'\n",
    "\n",
    "# val_dataset = ImageFolder(lambda_set, transform=transform_pipeline)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "pixel_score_set = ImageFolder(images_pixel_score_path, transform=transform_pipeline)\n",
    "pixel_score_loader = DataLoader(pixel_score_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_checkpoint_path = '/home/lcur1737/checkpoints/ffhq_continued_020.pt'\n",
    "ar_checkpoint_path = '/home/lcur1737/checkpoints/ffhq_ar_030.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "vq_model = nets_LV.VQVAE(\n",
    "    d=3,\n",
    "    n_channels=(16, 32, 64, 256),\n",
    "    code_size=128,\n",
    "    n_res_block=2,\n",
    "    dropout_p=.1\n",
    ").to(device)\n",
    "\n",
    "vqvae_checkpoint = torch.load(vqvae_checkpoint_path, map_location=device)\n",
    "vq_model.load_state_dict(vqvae_checkpoint[\"model\"])\n",
    "vq_model = vq_model.to(device)\n",
    "\n",
    "ar_model = nets_LV.VQLatentSNAIL(\n",
    "    feature_extractor_model=vq_model,\n",
    "    shape=(16, 16),\n",
    "    n_block=4,\n",
    "    n_res_block=4,\n",
    "    n_channels=128\n",
    ").to(device)\n",
    "\n",
    "ar_checkpoint = torch.load(ar_checkpoint_path, map_location=device)\n",
    "\n",
    "ar_model.load_state_dict(ar_checkpoint['model'])\n",
    "\n",
    "ar_model = ar_model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample wise score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "accuracy = []\n",
    "labels = []\n",
    "ar_model.eval()\n",
    "\n",
    "# 0 is fake\n",
    "# 1 is real\n",
    "thresholds = np.linspace(1,20,2)\n",
    "\n",
    "loaded_dataloader = tqdm(val_dataloader)\n",
    "print('Starting the measurements')\n",
    "for thr in thresholds:\n",
    "    print('for threshold', thr)\n",
    "    temp_preds = []\n",
    "    temp_labels = []\n",
    "    for batchX, batchY in loaded_dataloader:\n",
    "        batchX = batchX.to(device)\n",
    "        batchY = batchY.to(device)\n",
    "        with torch.no_grad():\n",
    "            loss = ar_model.loss(batchX, reduction='none')['loss'].flatten(1)\n",
    "\n",
    "            score = torch.sum(loss*(loss>thr), 1).float()\n",
    "            temp_preds.extend(score.cpu().numpy())\n",
    "            temp_labels.extend(batchY.cpu().numpy())\n",
    "\n",
    "    pred.append(temp_preds)\n",
    "    labels.append(temp_labels)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc_per_thr = []\n",
    "prec_per_thr = []\n",
    "big_threshold = []\n",
    "\n",
    "for threshold_idx, prediction in enumerate(pred):\n",
    "    pred_copy = np.array(copy.deepcopy(prediction))\n",
    "    real_preds = pred_copy[np.array(labels)[threshold_idx] == 1]\n",
    "    fake_preds = pred_copy[np.array(labels)[threshold_idx] == 0]\n",
    "    \n",
    "    max_real = np.max(real_preds)\n",
    "    min_fake = np.min(fake_preds)\n",
    "    \n",
    "    avg_thr = np.mean([max_real, min_fake])\n",
    "    big_threshold.append(avg_thr)\n",
    "    #Below are real\n",
    "    pred_copy[pred_copy <= avg_thr] = 1\n",
    "    \n",
    "    #Above are fake\n",
    "    pred_copy[pred_copy > avg_thr] = 0\n",
    "    acc_per_thr.append(accuracy_score(labels[threshold_idx], pred_copy))\n",
    "    prec_per_thr.append(precision_score(labels[threshold_idx], pred_copy))\n",
    "\n",
    "\n",
    "best_threshold = np.argmax(acc_per_thr)\n",
    "print('the mean for the big threshold:', np.mean(big_threshold))\n",
    "print('best threshold value is', thresholds[best_threshold])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find pixel score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(n, img, threshold_log_p = 5):\n",
    "    \"\"\" Generates n reconstructions for each image in img.\n",
    "    Resamples latent variables with cross-entropy > threshold\n",
    "    Returns corrected images and associated latent variables\"\"\"\n",
    "          \n",
    "    #Use VQ-VAE to encode original image\n",
    "    codes = ar_model.retrieve_codes(img)\n",
    "    code_size = codes.shape[-2:]\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        samples = codes.clone().unsqueeze(1).repeat(1,n,1,1).reshape(img.shape[0]*n,*code_size)\n",
    "\n",
    "        if not threshold_log_p == None:\n",
    "            for r in tqdm(range(code_size[0])):\n",
    "                for c in range(code_size[1]):        \n",
    "\n",
    "                    code_logits = ar_model.forward_latent(samples)[:,:,r,c]\n",
    "                    loss = F.cross_entropy(code_logits, samples[:, r, c], reduction='none')\n",
    "                    probs = F.softmax(code_logits, dim=1)\n",
    "\n",
    "                    samples[loss > threshold_log_p, r, c] = torch.multinomial(probs, 1).squeeze(-1)[loss > threshold_log_p]\n",
    "\n",
    "        z = vq_model.codebook.embedding(samples.unsqueeze(1))\n",
    "        z = z.squeeze(1).permute(0,3,1,2).contiguous()\n",
    "        \n",
    "        # Split the calculation in batches\n",
    "        x_tilde = []\n",
    "        for i in range(img.shape[0]):\n",
    "            x_tilde.append(vq_model.decode(z[i*n:(i+1)*n]))\n",
    "        x_tilde = torch.cat(x_tilde)\n",
    "        \n",
    "        \n",
    "    return x_tilde.reshape(img.shape[0]*img.shape[1],n,*img.shape[-2:]), samples.reshape(img.shape[0],n,*code_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 images experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(iter(pixel_score_loader))[0]\n",
    "reconstructionmax = reconstruct(n=5,img=X, threshold_log_p=None)[0]\n",
    "reconstruction8 = reconstruct(n=5,img=X, threshold_log_p=8)[0]\n",
    "reconstruction9 = reconstruct(n=5,img=X, threshold_log_p=9)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffrec = torch.abs(X[0] - torch.mean(reconstructionmax, dim=1))\n",
    "diff8 = torch.mean(torch.abs(reconstructionmax - reconstruction8), dim=1)\n",
    "diff9 = torch.mean(torch.abs(reconstructionmax - reconstruction9), dim=1)\n",
    "reconstructionmax = torch.mean(reconstructionmax, dim=1)\n",
    "reconstruction8 = torch.mean(reconstruction8, dim=1)\n",
    "reconstruction9 = torch.mean(reconstruction9, dim=1)\n",
    "\n",
    "fig, axes = plt.subplots(2,4, figsize=(20,10))\n",
    "\n",
    "axes[0,0].imshow(X[0].permute(1,2,0))\n",
    "axes[0,0].set_title('Original')\n",
    "axes[0,1].imshow(reconstructionmax.squeeze().permute(1,2,0))\n",
    "axes[0,1].set_title('No resampling')\n",
    "axes[0,2].imshow(reconstruction8.squeeze().permute(1,2,0))\n",
    "axes[0,2].set_title('Threshold 8')\n",
    "axes[0,3].imshow(reconstruction9.squeeze().permute(1,2,0))\n",
    "axes[0,3].set_title('Threshold 9')\n",
    "axes[1,1].imshow(diffrec.squeeze().permute(1,2,0))\n",
    "axes[1,1].set_title('|original - reconstruction|')\n",
    "axes[1,2].imshow(diff8.squeeze().permute(1,2,0))\n",
    "axes[1,2].set_title('|no resampling - threshold 8|')\n",
    "axes[1,3].imshow(diff9.squeeze().permute(1,2,0))\n",
    "axes[1,3].set_title('|no resampling - threshold 9|')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
