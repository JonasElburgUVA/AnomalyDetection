{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding\n",
    "This notebook will guide you through evaluating your models in order to find either the optimal threshold $\\lambda_s$, or to obtain final results. Before running this notebook, make sure your data is structured as described in readme.md, and the checkpoints are named {dataset}_{model}.pt (for dataset in ['ffhq', 'faceforensics'] and model in ['vqvae', 'ar'])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from ..utils import *\n",
    "import utils\n",
    "import nets_LV\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from sklearn.metrics import precision_score, accuracy_score, average_precision_score, roc_auc_score\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "txt_to_label = {'real': 0, 'fake': 1}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your paths and evaluation parameters here. If you want to find the optimal threshold, use the 'val' split and define thresholds as a list (e.g.: [5,6,7,8,9,10]). Set pruned to True to speed up the process. If you want to evaluate final results, use the 'test' split and define only one threshold as follows [$\\lambda$]. Set pruned to false when testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE DATASET HERE AND SET PATHS AS DESCRIBED, THEN NONE OF THE CODE BELOW NEEDS TO BE CHANGED\n",
    "data_dir = '../../data'\n",
    "checkpoint_path = 'checkpoints'\n",
    "dataset = 'faceforensics' # choose from ['ffhq', 'faceforensics']\n",
    "# mode = 'sample' # choose from ['sample', 'pixel']\n",
    "split = 'test' # choose from ['val', 'test']\n",
    "pruned = False # set to True if you want to use the smaller faceforensics dataset, e.g. for hyperparameter tuning\n",
    "thresholds = [7] # Only input 1 if testing\n",
    "difficulties = ['easy', 'medium', 'hard'] # only used for ffhq\n",
    "\n",
    "assert split in ['val', 'test']\n",
    "assert dataset in ['ffhq', 'faceforensics']\n",
    "assert set(difficulties).issubset(set(['easy', 'medium', 'hard']))\n",
    "\n",
    "if pruned:\n",
    "  pr=\"_pruned\"\n",
    "else:\n",
    "  pr=\"\"\n",
    "\n",
    "# For FFHQ, we create dataloaders here. For FaceForensics, data is loaded directly from the filesystem later.\n",
    "if dataset == \"ffhq\":\n",
    "  ffhq_sets = {}\n",
    "  ffhq_loaders = {}\n",
    "  for dif in difficulties:\n",
    "    ffhq_sets[dif]= ImageFolder(os.path.join(data_dir, dataset, split, dif), transform=transform)\n",
    "    ffhq_loaders[dif] = DataLoader(ffhq_sets[dif], batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_cp = torch.load(os.path.join(checkpoint_path, dataset + '_vqvae.pt'), map_location=device)\n",
    "ar_cp = torch.load(os.path.join(checkpoint_path, dataset + '_ar.pt'), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "vq_model = nets_LV.VQVAE(\n",
    "    d=3,\n",
    "    n_channels=(16, 32, 64, 256),\n",
    "    code_size=128,\n",
    "    n_res_block=2,\n",
    "    dropout_p=.1\n",
    ").to(device)\n",
    "\n",
    "vq_model.load_state_dict(vqvae_cp[\"model\"])\n",
    "vq_model = vq_model.to(device)\n",
    "\n",
    "ar_model = nets_LV.VQLatentSNAIL(\n",
    "    feature_extractor_model=vq_model,\n",
    "    shape=(16, 16),\n",
    "    n_block=4,\n",
    "    n_res_block=4,\n",
    "    n_channels=128\n",
    ").to(device)\n",
    "\n",
    "\n",
    "ar_model.load_state_dict(ar_cp['model'])\n",
    "ar_model = ar_model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate anomaly scores for various thresholds\n",
    "Here we use the vqvae model to make representations of images in the val/test set. These representations are assigned likelihoods by the ar-model, which are used to calculate anomaly scores. See the methods section of the blogpost for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# If you are using a Mac, run this command to delete the .DS_Store files\n",
    "# !find ../../data -name \".DS_Store\" -type f -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating {dataset} with thresholds {thresholds}\")\n",
    "input_dir = data_dir + f'/{dataset}'\n",
    "output_dir = data_dir + '/output'\n",
    "\n",
    "# bruh moment right here\n",
    "predictions = defaultdict(lambda: defaultdict())\n",
    "\n",
    "\n",
    "if dataset == \"ffhq\":\n",
    "    for thr in thresholds:\n",
    "        predictions[thr] = {}\n",
    "        with torch.no_grad():\n",
    "          for dif in difficulties:\n",
    "            predictions[thr][dif] = {'scores':[],'labels':[]}\n",
    "            for batch, cl in tqdm(ffhq_loaders[dif]):\n",
    "              loss = ar_model.loss(batch.to(device), reduction=\"none\")[\"loss\"].flatten(1)\n",
    "              scores = torch.sum(loss * (loss > thr), 1).float()\n",
    "              predictions[thr][dif]['scores'].append(scores.detach().cpu().numpy().tolist())\n",
    "              predictions[thr][dif]['labels'].append(cl.detach().cpu().numpy().tolist())\n",
    "            assert len(predictions[thr][dif]['scores']) == len(predictions[thr][dif]['labels'])\n",
    "    with open(os.path.join(output_dir, \"ffhq\", \"scores.json\"), \"w\") as write_file:\n",
    "        json.dump(predictions, write_file)\n",
    "\n",
    "\n",
    "# mean version\n",
    "elif dataset == \"faceforensics\":\n",
    "   input_dir = os.path.join(data_dir, dataset, f'{split}_set_pruned')\n",
    "   for thr in thresholds:\n",
    "      predictions[thr] = {'scores':[],'labels':[]}\n",
    "      with torch.no_grad():\n",
    "          for rf in os.listdir(input_dir):\n",
    "            for img_id in tqdm(os.listdir(os.path.join(input_dir, rf))):\n",
    "              imgdir = os.listdir(os.path.join(input_dir, rf, img_id)) \n",
    "              batch = torch.stack([transform(Image.open(os.path.join(input_dir, rf, img_id, img))) for img in imgdir]).to(device)\n",
    "              loss = ar_model.loss(batch, reduction=\"none\")[\"loss\"].flatten(1)\n",
    "              scores = torch.sum(loss * (loss > thr), 1).float()\n",
    "              score = scores.mean()\n",
    "              predictions[thr]['scores'].append(score.detach().cpu().numpy().tolist())\n",
    "              predictions[thr]['labels'].append(txt_to_label[rf])\n",
    "\n",
    "with open(os.path.join(output_dir, \"faceforensics\",\"scores.json\"), \"w\") as write_file:\n",
    "        json.dump(predictions, write_file)\n",
    "               \n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results per threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir + f\"/{dataset}/scores.json\") as s:\n",
    "    scores = json.load(s)\n",
    "\n",
    "\n",
    "if dataset == 'ffhq':\n",
    "  res = defaultdict(lambda: defaultdict(lambda: {'auroc': None, 'ap': None}))\n",
    "  for thr in thresholds:\n",
    "    thr = str(thr)\n",
    "    for dif in ['easy', 'hard']:\n",
    "      labels = np.concatenate(scores[thr][dif]['labels'])\n",
    "      preds = np.concatenate(scores[thr][dif]['scores'])\n",
    "\n",
    "      auroc = roc_auc_score(labels,preds)\n",
    "      ap = average_precision_score(labels,preds, pos_label=1)\n",
    "\n",
    "      res[thr][dif]['auroc'] = auroc\n",
    "      res[thr][dif]['ap'] = ap\n",
    "\n",
    "elif dataset == 'faceforensics':\n",
    "  res = defaultdict(lambda: {'auroc' : None, 'ap': None})\n",
    "  for thr in thresholds:\n",
    "    thr = str(thr)\n",
    "    labels = scores[thr]['labels']\n",
    "    preds = scores[thr]['scores']\n",
    "\n",
    "    auroc = roc_auc_score(labels,preds)\n",
    "    ap = average_precision_score(labels,preds, pos_label=1)\n",
    "\n",
    "    res[thr]['auroc'] = auroc\n",
    "    res[thr]['ap'] = ap\n",
    "\n",
    "for thr in thresholds:\n",
    "  print(f\"threshold = {thr}\")\n",
    "  print(res[str(thr)].items())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
