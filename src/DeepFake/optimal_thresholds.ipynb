{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding\n",
    "This notebook will guide you through finding the optimal threshold hyperparameters for your data and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from ..utils import *\n",
    "import utils\n",
    "import nets_LV\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from sklearn.metrics import precision_score, accuracy_score, average_precision_score, roc_auc_score\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your paths here. Make sure your data is structured as follows:\n",
    "\n",
    "- data\n",
    "    - FaceForensics\n",
    "        - test_set\n",
    "            - fake\n",
    "                - 000_003\n",
    "                    - face_0_0_10.jpg\n",
    "                    - ...\n",
    "                - ...\n",
    "            - real\n",
    "                - 000\n",
    "                - ...\n",
    "         - validation_set\n",
    "            - ...\n",
    "    - FFHQ\n",
    "        - test_set\n",
    "            - easy\n",
    "                - fake\n",
    "                    - 1_1110.png\n",
    "                    - ...\n",
    "                - real\n",
    "                    - 51010.png\n",
    "                    - ...\n",
    "            - medium\n",
    "                - ...\n",
    "            - hard\n",
    "                - ...\n",
    "        - validation_set\n",
    "            - ...\n",
    "\n",
    "and the checkpoints are named {dataset}_{model}.pt for dataset in ['ffhq', 'faceforensics'] and model in ['vqvae', 'ar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE DATASET HERE AND SET PATHS AS DESCRIBED, THEN NONE OF THE CODE BELOW NEEDS TO BE CHANGED\n",
    "data_dir = '../../data'\n",
    "checkpoint_path = 'checkpoints'\n",
    "dataset = 'ffhq' # choose from ['ffhq', 'faceforensics']\n",
    "\n",
    "# val_dataset = ImageFolder(lambda_set, transform=transform_pipeline)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "# pixel_score_set = ImageFolder(images_pixel_score_path, transform=transform_pipeline)\n",
    "# pixel_score_loader = DataLoader(pixel_score_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_cp = torch.load(os.path.join(checkpoint_path, dataset + '_vqvae.pt'), map_location=device)\n",
    "ar_cp = torch.load(os.path.join(checkpoint_path, dataset + '_ar.pt'), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "vq_model = nets_LV.VQVAE(\n",
    "    d=3,\n",
    "    n_channels=(16, 32, 64, 256),\n",
    "    code_size=128,\n",
    "    n_res_block=2,\n",
    "    dropout_p=.1\n",
    ").to(device)\n",
    "\n",
    "vq_model.load_state_dict(vqvae_cp[\"model\"])\n",
    "vq_model = vq_model.to(device)\n",
    "\n",
    "ar_model = nets_LV.VQLatentSNAIL(\n",
    "    feature_extractor_model=vq_model,\n",
    "    shape=(16, 16),\n",
    "    n_block=4,\n",
    "    n_res_block=4,\n",
    "    n_channels=128\n",
    ").to(device)\n",
    "\n",
    "\n",
    "ar_model.load_state_dict(ar_cp['model'])\n",
    "ar_model = ar_model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate anomaly scores for various thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!find ../../data -name \".DS_Store\" -type f -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresholds = np.linspace(5,15,3)\n",
    "print(thresholds)\n",
    "input_dir = data_dir + f'/{dataset}'\n",
    "output_dir = data_dir + '/output'\n",
    "\n",
    "# bruh moment right here\n",
    "predictions = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None))))\n",
    "\n",
    "if dataset == \"ffhq\":\n",
    "    for thr in thresholds:\n",
    "        with torch.no_grad():            \n",
    "            for difficulty in os.listdir(input_dir):\n",
    "                for real_or_fake in os.listdir(os.path.join(input_dir, difficulty)):\n",
    "                    for file_id in tqdm(os.listdir(os.path.join(input_dir, difficulty, real_or_fake))):\n",
    "                        img = Image.open(os.path.join(input_dir, difficulty, real_or_fake, file_id))\n",
    "                        img = torch.unsqueeze(transform(img).to(device), 0)\n",
    "                        loss = ar_model.loss(img, reduction=\"none\")[\"loss\"].flatten(1)\n",
    "                        scores = torch.sum(loss * (loss > thr), 1).float()\n",
    "                        score = scores.sum()\n",
    "                        predictions[thr][difficulty][real_or_fake][file_id] = score.detach().cpu().numpy().tolist()\n",
    "    with open(os.path.join(output_dir, \"ffhq\", \"scores.json\"), \"w\") as write_file:\n",
    "        json.dump(predictions, write_file)\n",
    "\n",
    "elif dataset == \"faceforensics\":\n",
    "    input_dir = data_dir + '/faceforensics/test_set'\n",
    "    for thr in thresholds:\n",
    "        with torch.no_grad():\n",
    "            for real_or_fake in os.listdir(input_dir):\n",
    "                for vid in tqdm(os.listdir(os.path.join(input_dir, real_or_fake))):\n",
    "                    file_list = os.listdir(os.path.join(input_dir, real_or_fake, vid))\n",
    "                    for file_id in file_list[:min(10, len(file_list))]: # maybe batching will be faster here\n",
    "                        img = Image.open(os.path.join(input_dir, real_or_fake, vid, file_id))\n",
    "                        img = torch.unsqueeze(transform(img).to(device), 0)\n",
    "                        loss = ar_model.loss(img, reduction=\"none\")[\"loss\"].flatten(1)\n",
    "                        scores = torch.sum(loss * (loss > thr), 1).float()\n",
    "                        score = scores.sum()\n",
    "                        predictions[thr][real_or_fake][vid][file_id] = score.detach().cpu().numpy().tolist()\n",
    "    with open(os.path.join(output_dir, \"faceforensics\",\"scores_sample.json\"), \"w\") as write_file:\n",
    "        json.dump(predictions, write_file)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample wise score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir + f\"/{dataset}/scores_sample.json\") as s:\n",
    "    scores = json.load(s)\n",
    "\n",
    "# I know this looks awful but if the paths are set up correctly it will run.\n",
    "# Probably better with pandas but I don't have time rn.\n",
    "res = defaultdict(lambda: {'auroc': None, 'ap': None})\n",
    "fake_ids = list(scores[thr]['0'].keys())\n",
    "real_ids = list(scores[thr]['1'].keys())\n",
    "n_fake = len(np.concatenate([list(scores[thr]['0'][id].values()) for id in fake_ids]))\n",
    "n_real = len(np.concatenate([list(scores[thr]['1'][id].values()) for id in real_ids]))\n",
    "labels = n_fake*[1] + n_real*[0]\n",
    "\n",
    "for thr in scores.keys():\n",
    "    pred = np.concatenate([list(scores[thr]['0'][id].values()) for id in fake_ids] + [list(scores[thr]['1'][id].values()) for id in real_ids])\n",
    "    auroc = roc_auc_score(labels, pred)\n",
    "    res[thr]['auroc'] = auroc\n",
    "    ap = average_precision_score(labels, pred)\n",
    "    res[thr]['ap'] = ap\n",
    "\n",
    "for item in res.items():\n",
    "    print(item)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to save the best threshold!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "accuracy = []\n",
    "labels = []\n",
    "ar_model.eval()\n",
    "\n",
    "# 0 is fake\n",
    "# 1 is real\n",
    "thresholds = np.linspace(1,20,2)\n",
    "\n",
    "loaded_dataloader = tqdm(val_dataloader)\n",
    "print('Starting the measurements')\n",
    "for thr in thresholds:\n",
    "    print('for threshold', thr)\n",
    "    temp_preds = []\n",
    "    temp_labels = []\n",
    "    for batchX, batchY in loaded_dataloader:\n",
    "        batchX = batchX.to(device)\n",
    "        batchY = batchY.to(device)\n",
    "        with torch.no_grad():\n",
    "            loss = ar_model.loss(batchX, reduction='none')['loss'].flatten(1)\n",
    "\n",
    "            score = torch.sum(loss*(loss>thr), 1).float()\n",
    "            temp_preds.extend(score.cpu().numpy())\n",
    "            temp_labels.extend(batchY.cpu().numpy())\n",
    "\n",
    "    pred.append(temp_preds)\n",
    "    labels.append(temp_labels)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc_per_thr = []\n",
    "prec_per_thr = []\n",
    "big_threshold = []\n",
    "\n",
    "for threshold_idx, prediction in enumerate(pred):\n",
    "    pred_copy = np.array(copy.deepcopy(prediction))\n",
    "    real_preds = pred_copy[np.array(labels)[threshold_idx] == 1]\n",
    "    fake_preds = pred_copy[np.array(labels)[threshold_idx] == 0]\n",
    "    \n",
    "    max_real = np.max(real_preds)\n",
    "    min_fake = np.min(fake_preds)\n",
    "    \n",
    "    avg_thr = np.mean([max_real, min_fake])\n",
    "    big_threshold.append(avg_thr)\n",
    "    #Below are real\n",
    "    pred_copy[pred_copy <= avg_thr] = 1\n",
    "    \n",
    "    #Above are fake\n",
    "    pred_copy[pred_copy > avg_thr] = 0\n",
    "    acc_per_thr.append(accuracy_score(labels[threshold_idx], pred_copy))\n",
    "    prec_per_thr.append(precision_score(labels[threshold_idx], pred_copy))\n",
    "\n",
    "\n",
    "best_threshold = np.argmax(acc_per_thr)\n",
    "print('the mean for the big threshold:', np.mean(big_threshold))\n",
    "print('best threshold value is', thresholds[best_threshold])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find pixel score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(n, img, threshold_log_p = 5):\n",
    "    \"\"\" Generates n reconstructions for each image in img.\n",
    "    Resamples latent variables with cross-entropy > threshold\n",
    "    Returns corrected images and associated latent variables\"\"\"\n",
    "          \n",
    "    #Use VQ-VAE to encode original image\n",
    "    codes = ar_model.retrieve_codes(img)\n",
    "    code_size = codes.shape[-2:]\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        samples = codes.clone().unsqueeze(1).repeat(1,n,1,1).reshape(img.shape[0]*n,*code_size)\n",
    "\n",
    "        if not threshold_log_p == None:\n",
    "            for r in tqdm(range(code_size[0])):\n",
    "                for c in range(code_size[1]):        \n",
    "\n",
    "                    code_logits = ar_model.forward_latent(samples)[:,:,r,c]\n",
    "                    loss = F.cross_entropy(code_logits, samples[:, r, c], reduction='none')\n",
    "                    probs = F.softmax(code_logits, dim=1)\n",
    "\n",
    "                    samples[loss > threshold_log_p, r, c] = torch.multinomial(probs, 1).squeeze(-1)[loss > threshold_log_p]\n",
    "\n",
    "        z = vq_model.codebook.embedding(samples.unsqueeze(1))\n",
    "        z = z.squeeze(1).permute(0,3,1,2).contiguous()\n",
    "        \n",
    "        # Split the calculation in batches\n",
    "        x_tilde = []\n",
    "        for i in range(img.shape[0]):\n",
    "            x_tilde.append(vq_model.decode(z[i*n:(i+1)*n]))\n",
    "        x_tilde = torch.cat(x_tilde)\n",
    "        \n",
    "        \n",
    "    return x_tilde.reshape(img.shape[0]*img.shape[1],n,*img.shape[-2:]), samples.reshape(img.shape[0],n,*code_size)\n",
    "\n",
    "reconstructions = []\n",
    "\n",
    "# for X,_ in loaded_dataloader:\n",
    "#     x_tilde, latent_sample = reconstruct(n=15,img=X, threshold_log_p=5)\n",
    "#     reconstructions.append(x_tilde)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 images experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(iter(pixel_score_loader))[0]\n",
    "reconstructionmax = reconstruct(n=5,img=X, threshold_log_p=None)[0]\n",
    "reconstruction8 = reconstruct(n=5,img=X, threshold_log_p=8)[0]\n",
    "reconstruction9 = reconstruct(n=5,img=X, threshold_log_p=9)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffrec = torch.abs(X[0] - torch.mean(reconstructionmax, dim=1))\n",
    "diff8 = torch.mean(torch.abs(reconstructionmax - reconstruction8), dim=1)\n",
    "diff9 = torch.mean(torch.abs(reconstructionmax - reconstruction9), dim=1)\n",
    "reconstructionmax = torch.mean(reconstructionmax, dim=1)\n",
    "reconstruction8 = torch.mean(reconstruction8, dim=1)\n",
    "reconstruction9 = torch.mean(reconstruction9, dim=1)\n",
    "\n",
    "fig, axes = plt.subplots(2,4, figsize=(20,10))\n",
    "\n",
    "axes[0,0].imshow(X[0].permute(1,2,0))\n",
    "axes[0,0].set_title('Original')\n",
    "axes[0,1].imshow(reconstructionmax.squeeze().permute(1,2,0))\n",
    "axes[0,1].set_title('No resampling')\n",
    "axes[0,2].imshow(reconstruction8.squeeze().permute(1,2,0))\n",
    "axes[0,2].set_title('Threshold 8')\n",
    "axes[0,3].imshow(reconstruction9.squeeze().permute(1,2,0))\n",
    "axes[0,3].set_title('Threshold 9')\n",
    "axes[1,1].imshow(diffrec.squeeze().permute(1,2,0))\n",
    "axes[1,1].set_title('|original - reconstruction|')\n",
    "axes[1,2].imshow(diff8.squeeze().permute(1,2,0))\n",
    "axes[1,2].set_title('|no resampling - threshold 8|')\n",
    "axes[1,3].imshow(diff9.squeeze().permute(1,2,0))\n",
    "axes[1,3].set_title('|no resampling - threshold 9|')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
