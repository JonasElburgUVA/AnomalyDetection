{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding\n",
    "This notebook will guide you through finding the optimal threshold hyperparameters for your data and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjonas-van-elburg\u001b[0m (\u001b[33mvqvaeanomaly\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/Users/jonase/opt/miniconda3/envs/lsr_mood/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/jonase/opt/miniconda3/envs/lsr_mood/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: (__ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE)\n",
      "  Referenced from: '/Users/jonase/opt/miniconda3/envs/lsr_mood/lib/python3.9/site-packages/torchvision/image.so'\n",
      "  Expected in: '/Users/jonase/opt/miniconda3/envs/lsr_mood/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib'\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from ..utils import *\n",
    "import utils\n",
    "import nets_LV\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from sklearn.metrics import precision_score, accuracy_score, average_precision_score, roc_auc_score\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your paths here. Make sure your data is structured as follows:\n",
    "\n",
    "- data\n",
    "    - FaceForensics\n",
    "        - test_set\n",
    "            - fake\n",
    "                - 000_003\n",
    "                    - face_0_0_10.jpg\n",
    "                    - ...\n",
    "                - ...\n",
    "            - real\n",
    "                - 000\n",
    "                - ...\n",
    "         - validation_set\n",
    "            - ...\n",
    "    - FFHQ\n",
    "        - test_set\n",
    "            - easy\n",
    "                - 1 # indicating fake\n",
    "                    - 1_1110.png\n",
    "                    - ...\n",
    "                - 0 # indicating real\n",
    "                    - 51010.png\n",
    "                    - ...\n",
    "            - medium\n",
    "                - ...\n",
    "            - hard\n",
    "                - ...\n",
    "        - validation_set\n",
    "            - ...\n",
    "    - Output\n",
    "        - faceforensics\n",
    "        - ffhq\n",
    "    - Checkpoints\n",
    "        - ffhq_vqvae.pt\n",
    "        - ffhq_ar.pt\n",
    "        - faceforensics_vqvae.pt\n",
    "        - faceforensics_ar.pt\n",
    "\n",
    "and the checkpoints are named {dataset}_{model}.pt for dataset in ['ffhq', 'faceforensics'] and model in ['vqvae', 'ar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE DATASET HERE AND SET PATHS AS DESCRIBED, THEN NONE OF THE CODE BELOW NEEDS TO BE CHANGED\n",
    "data_dir = '../../data'\n",
    "checkpoint_path = 'checkpoints'\n",
    "dataset = 'faceforensics' # choose from ['ffhq', 'faceforensics']\n",
    "mode = 'sample' # choose from ['sample', 'pixel']\n",
    "split = 'val' # choose from ['val', 'test']\n",
    "test = False # choose from [True, False]. If False, the test set is used\n",
    "thresholds = [2, 4, 6, 8] # Only input 1 if testing\n",
    "difficulties = ['easy', 'medium', 'hard'] # only used for ffhq\n",
    "assert dataset in ['ffhq', 'faceforensics']\n",
    "txt_to_label = {'real': 0, 'fake': 1}\n",
    "\n",
    "if dataset == 'faceforensics':\n",
    "    ff_sets = {}\n",
    "    ff_loaders = {}\n",
    "    for fr in ['real', 'fake']:\n",
    "      ff_sets[fr] = ImageFolder(os.path.join(data_dir, dataset, f'{split}_set_pruned', fr), transform=transform)\n",
    "      ff_loaders[fr] = DataLoader(ff_sets[fr], batch_size=64, shuffle=False)\n",
    "\n",
    "elif dataset == \"ffhq\":\n",
    "    ffhq_test_sets = {}\n",
    "    ffhq_test_loaders = {}\n",
    "    for dif in difficulties:\n",
    "      ffhq_test_sets[dif]= ImageFolder(os.path.join(data_dir, dataset, split, dif), transform=transform)\n",
    "      ffhq_test_loaders[dif] = DataLoader(ffhq_test_sets[dif], batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_cp = torch.load(os.path.join(checkpoint_path, dataset + '_vqvae.pt'), map_location=device)\n",
    "ar_cp = torch.load(os.path.join(checkpoint_path, dataset + '_ar.pt'), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "vq_model = nets_LV.VQVAE(\n",
    "    d=3,\n",
    "    n_channels=(16, 32, 64, 256),\n",
    "    code_size=128,\n",
    "    n_res_block=2,\n",
    "    dropout_p=.1\n",
    ").to(device)\n",
    "\n",
    "vq_model.load_state_dict(vqvae_cp[\"model\"])\n",
    "vq_model = vq_model.to(device)\n",
    "\n",
    "ar_model = nets_LV.VQLatentSNAIL(\n",
    "    feature_extractor_model=vq_model,\n",
    "    shape=(16, 16),\n",
    "    n_block=4,\n",
    "    n_res_block=4,\n",
    "    n_channels=128\n",
    ").to(device)\n",
    "\n",
    "\n",
    "ar_model.load_state_dict(ar_cp['model'])\n",
    "ar_model = ar_model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate anomaly scores for various thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!find ../../data -name \".DS_Store\" -type f -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1736643887.py, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 31\u001b[0;36m\u001b[0m\n\u001b[0;31m    elif dataset == \"faceforensics\":\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating {dataset} with thresholds {thresholds}\")\n",
    "input_dir = data_dir + f'/{dataset}'\n",
    "output_dir = data_dir + '/output'\n",
    "\n",
    "# bruh moment right here\n",
    "predictions = defaultdict(lambda: defaultdict())\n",
    "\n",
    "\n",
    "if dataset == \"ffhq\":\n",
    "    for thr in thresholds:\n",
    "        predictions[thr] = {}\n",
    "        with torch.no_grad():\n",
    "          for dif in ['easy', 'hard']:\n",
    "            predictions[thr][dif] = {'scores':[],'labels':[]}\n",
    "            for batch, cl in tqdm(ffhq_test_loaders[dif]):\n",
    "              loss = ar_model.loss(batch.to(device), reduction=\"none\")[\"loss\"].flatten(1)\n",
    "              scores = torch.sum(loss * (loss > thr), 1).float()\n",
    "              predictions[thr][dif]['scores'].append(scores.detach().cpu().numpy().tolist())\n",
    "              predictions[thr][dif]['labels'].append(cl.detach().cpu().numpy().tolist())\n",
    "            assert len(predictions[thr][dif]['scores']) == len(predictions[thr][dif]['labels'])\n",
    "    with open(os.path.join(output_dir, \"ffhq\", \"scores.json\"), \"w\") as write_file:\n",
    "        json.dump(predictions, write_file)\n",
    "\n",
    "\n",
    "elif dataset == \"faceforensics\":\n",
    "    input_dir = os.path.join(data_dir, dataset, f'{split}_set_pruned')\n",
    "    for thr in thresholds:\n",
    "        predictions[thr]['scores'] = []\n",
    "        predictions[thr]['labels'] = []\n",
    "        with torch.no_grad():\n",
    "            for rf in os.listdir(input_dir):\n",
    "              label = txt_to_label[rf] # 0 for real, 1 for fake, hacky way to get the labels because imagefolder doesn't give the correct labels\n",
    "              loader = ff_loaders[rf]\n",
    "              for t, (batch, _) in enumerate(tqdm(loader)):\n",
    "                  if t > 4:\n",
    "                     continue\n",
    "                  batch = batch.to(device)\n",
    "                  loss = ar_model.loss(batch, reduction=\"none\")[\"loss\"].flatten(1)\n",
    "                  scores = torch.sum(loss * (loss > thr), 1).float()\n",
    "                  labels = [label] * len(scores)\n",
    "                  predictions[thr]['scores'].append(scores.detach().cpu().numpy().tolist())\n",
    "                  predictions[thr]['labels'].append(labels)\n",
    "    # make sure your output directory exists before running                  \n",
    "    with open(os.path.join(output_dir, \"faceforensics\",\"scores.json\"), \"w\") as write_file:\n",
    "        json.dump(predictions, write_file)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample wise score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('7', defaultdict(<function <lambda>.<locals>.<lambda> at 0x13a6b60d0>, {'easy': {'auroc': 0.051867219917012444, 'ap': 0.9977379077086245}, 'hard': {'auroc': 0.1584375, 'ap': 0.8615225794547787}})), ('8', defaultdict(<function <lambda>.<locals>.<lambda> at 0x16ea6ca60>, {'easy': {'auroc': 0.11659751037344399, 'ap': 0.9947267227046538}, 'hard': {'auroc': 0.18838541666666667, 'ap': 0.829091633201088}}))])\n"
     ]
    }
   ],
   "source": [
    "with open(output_dir + f\"/{dataset}/scores.json\") as s:\n",
    "    scores = json.load(s)\n",
    "\n",
    "\n",
    "if dataset == 'ffhq':\n",
    "  res = defaultdict(lambda: defaultdict(lambda: {'auroc': None, 'ap': None}))\n",
    "  for thr in thresholds:\n",
    "    thr = str(thr)\n",
    "    for dif in ['easy', 'hard']:\n",
    "      labels = np.concatenate(scores[thr][dif]['labels'])\n",
    "      preds = np.concatenate(scores[thr][dif]['scores'])\n",
    "\n",
    "      auroc = roc_auc_score(labels,preds)\n",
    "      ap = average_precision_score(labels,preds, pos_label=1)\n",
    "\n",
    "      res[thr][dif]['auroc'] = auroc\n",
    "      res[thr][dif]['ap'] = ap\n",
    "\n",
    "elif dataset == 'faceforensics':\n",
    "  res = defaultdict(lambda: {'auroc' : None, 'ap': None})\n",
    "  for thr in thresholds:\n",
    "    thr = str(thr)\n",
    "    labels = np.concatenate(scores[thr]['labels'])\n",
    "    preds = np.concatenate(scores[thr]['scores'])\n",
    "\n",
    "    auroc = roc_auc_score(labels,preds)\n",
    "    ap = average_precision_score(labels,preds, pos_label=1)\n",
    "\n",
    "    res[thr]['auroc'] = auroc\n",
    "    res[thr]['ap'] = ap\n",
    "\n",
    "print(res.items())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to save the best threshold!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD STUFF BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5. 10. 15.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 6/327 [01:40<1:29:11, 16.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m cl \u001b[39m=\u001b[39m cl\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[39m# img = Image.open(os.path.join(input_dir, real_or_fake, vid, file_id))\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# img = torch.unsqueeze(transform(img).to(device), 0)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m loss \u001b[39m=\u001b[39m ar_model\u001b[39m.\u001b[39;49mloss(batch, reduction\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnone\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mflatten(\u001b[39m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(loss \u001b[39m*\u001b[39m (loss \u001b[39m>\u001b[39m thr), \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     38\u001b[0m score \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39msum()\n",
      "File \u001b[0;32m~/Documents/UVA/DEL2/AnomalyDetection/AnomalyDetection/src/DeepFake/nets_LV.py:166\u001b[0m, in \u001b[0;36mVQLatentSNAIL.loss\u001b[0;34m(self, x, reduction)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39mself\u001b[39m, x, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Retrieve codes for images\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretrieve_codes(x)\n\u001b[0;32m--> 166\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(VQLatentSNAIL, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mforward(code)\n\u001b[1;32m    167\u001b[0m     nll \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(logits, code, reduction\u001b[39m=\u001b[39mreduction)\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m OrderedDict(loss\u001b[39m=\u001b[39mnll)\n",
      "File \u001b[0;32m~/Documents/UVA/DEL2/AnomalyDetection/AnomalyDetection/src/DeepFake/nets_AR.py:55\u001b[0m, in \u001b[0;36mPixelSNAIL.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mini_conv(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m---> 55\u001b[0m     out \u001b[39m=\u001b[39m block(out)\n\u001b[1;32m     57\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsample(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnon_linearity(out))\n\u001b[1;32m     58\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnon_linearity(out))\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/lsr_mood/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/UVA/DEL2/AnomalyDetection/AnomalyDetection/src/DeepFake/nn_blocks.py:314\u001b[0m, in \u001b[0;36mPixelBlock.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    311\u001b[0m attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcausal_attention(query, key)\n\u001b[1;32m    312\u001b[0m attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsample(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnon_linearity(attn_out))\n\u001b[0;32m--> 314\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_resblock(out, attn_out)\n\u001b[1;32m    316\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/lsr_mood/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/UVA/DEL2/AnomalyDetection/AnomalyDetection/src/DeepFake/nn_blocks.py:178\u001b[0m, in \u001b[0;36mGatedResNet.forward\u001b[0;34m(self, x, aux)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_p\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m    176\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(out)\n\u001b[0;32m--> 178\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(out)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(out)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/lsr_mood/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/UVA/DEL2/AnomalyDetection/AnomalyDetection/src/DeepFake/nn_blocks.py:14\u001b[0m, in \u001b[0;36mWNConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/lsr_mood/lib/python3.9/site-packages/torch/nn/modules/module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/lsr_mood/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/lsr_mood/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "thresholds = np.linspace(5,15,3)\n",
    "print(thresholds)\n",
    "input_dir = data_dir + f'/{dataset}'\n",
    "output_dir = data_dir + '/output'\n",
    "\n",
    "# bruh moment right here\n",
    "predictions = defaultdict(lambda: defaultdict())\n",
    "\n",
    "if dataset == \"ffhq\":\n",
    "    for thr in thresholds:\n",
    "        with torch.no_grad():            \n",
    "            for difficulty in os.listdir(input_dir):\n",
    "                for real_or_fake in os.listdir(os.path.join(input_dir, difficulty)):\n",
    "                    for file_id in tqdm(os.listdir(os.path.join(input_dir, difficulty, real_or_fake))):\n",
    "                        img = Image.open(os.path.join(input_dir, difficulty, real_or_fake, file_id))\n",
    "                        img = torch.unsqueeze(transform(img).to(device), 0)\n",
    "                        loss = ar_model.loss(img, reduction=\"none\")[\"loss\"].flatten(1)\n",
    "                        scores = torch.sum(loss * (loss > thr), 1).float()\n",
    "                        score = scores.sum()\n",
    "                        predictions[thr][difficulty][real_or_fake][file_id] = score.detach().cpu().numpy().tolist()\n",
    "    with open(os.path.join(output_dir, \"ffhq\", \"scores.json\"), \"w\") as write_file:\n",
    "        json.dump(predictions, write_file)\n",
    "\n",
    "# 0 = real, 1 = fake\n",
    "elif dataset == \"faceforensics\":\n",
    "    input_dir = data_dir + '/faceforensics/test_set'\n",
    "    for thr in thresholds:\n",
    "        predictions[thr][0] = []\n",
    "        predictions[thr][1] = []\n",
    "        with torch.no_grad():\n",
    "            for batch, cl in tqdm(test_dataloader): \n",
    "                # assert all(x == cl[0] for x in cl)\n",
    "                cl = cl.detach().cpu().numpy().tolist()[0]\n",
    "                # img = Image.open(os.path.join(input_dir, real_or_fake, vid, file_id))\n",
    "                # img = torch.unsqueeze(transform(img).to(device), 0)\n",
    "                loss = ar_model.loss(batch, reduction=\"none\")[\"loss\"].flatten(1)\n",
    "                scores = torch.sum(loss * (loss > thr), 1).float()\n",
    "                score = scores.sum()\n",
    "                predictions[thr][cl].append(score.detach().cpu().numpy().tolist())\n",
    "    with open(os.path.join(output_dir, \"faceforensics\",\"scores_sample.json\"), \"w\") as write_file:\n",
    "        json.dump(predictions, write_file)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "accuracy = []\n",
    "labels = []\n",
    "ar_model.eval()\n",
    "\n",
    "# 0 is fake\n",
    "# 1 is real\n",
    "thresholds = np.linspace(1,20,2)\n",
    "\n",
    "loaded_dataloader = tqdm(val_dataloader)\n",
    "print('Starting the measurements')\n",
    "for thr in thresholds:\n",
    "    print('for threshold', thr)\n",
    "    temp_preds = []\n",
    "    temp_labels = []\n",
    "    for batchX, batchY in loaded_dataloader:\n",
    "        batchX = batchX.to(device)\n",
    "        batchY = batchY.to(device)\n",
    "        with torch.no_grad():\n",
    "            loss = ar_model.loss(batchX, reduction='none')['loss'].flatten(1)\n",
    "\n",
    "            score = torch.sum(loss*(loss>thr), 1).float()\n",
    "            temp_preds.extend(score.cpu().numpy())\n",
    "            temp_labels.extend(batchY.cpu().numpy())\n",
    "\n",
    "    pred.append(temp_preds)\n",
    "    labels.append(temp_labels)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc_per_thr = []\n",
    "prec_per_thr = []\n",
    "big_threshold = []\n",
    "\n",
    "for threshold_idx, prediction in enumerate(pred):\n",
    "    pred_copy = np.array(copy.deepcopy(prediction))\n",
    "    real_preds = pred_copy[np.array(labels)[threshold_idx] == 1]\n",
    "    fake_preds = pred_copy[np.array(labels)[threshold_idx] == 0]\n",
    "    \n",
    "    max_real = np.max(real_preds)\n",
    "    min_fake = np.min(fake_preds)\n",
    "    \n",
    "    avg_thr = np.mean([max_real, min_fake])\n",
    "    big_threshold.append(avg_thr)\n",
    "    #Below are real\n",
    "    pred_copy[pred_copy <= avg_thr] = 1\n",
    "    \n",
    "    #Above are fake\n",
    "    pred_copy[pred_copy > avg_thr] = 0\n",
    "    acc_per_thr.append(accuracy_score(labels[threshold_idx], pred_copy))\n",
    "    prec_per_thr.append(precision_score(labels[threshold_idx], pred_copy))\n",
    "\n",
    "\n",
    "best_threshold = np.argmax(acc_per_thr)\n",
    "print('the mean for the big threshold:', np.mean(big_threshold))\n",
    "print('best threshold value is', thresholds[best_threshold])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find pixel score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(n, img, threshold_log_p = 5):\n",
    "    \"\"\" Generates n reconstructions for each image in img.\n",
    "    Resamples latent variables with cross-entropy > threshold\n",
    "    Returns corrected images and associated latent variables\"\"\"\n",
    "          \n",
    "    #Use VQ-VAE to encode original image\n",
    "    codes = ar_model.retrieve_codes(img)\n",
    "    code_size = codes.shape[-2:]\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        samples = codes.clone().unsqueeze(1).repeat(1,n,1,1).reshape(img.shape[0]*n,*code_size)\n",
    "\n",
    "        if not threshold_log_p == None:\n",
    "            for r in tqdm(range(code_size[0])):\n",
    "                for c in range(code_size[1]):        \n",
    "\n",
    "                    code_logits = ar_model.forward_latent(samples)[:,:,r,c]\n",
    "                    loss = F.cross_entropy(code_logits, samples[:, r, c], reduction='none')\n",
    "                    probs = F.softmax(code_logits, dim=1)\n",
    "\n",
    "                    samples[loss > threshold_log_p, r, c] = torch.multinomial(probs, 1).squeeze(-1)[loss > threshold_log_p]\n",
    "\n",
    "        z = vq_model.codebook.embedding(samples.unsqueeze(1))\n",
    "        z = z.squeeze(1).permute(0,3,1,2).contiguous()\n",
    "        \n",
    "        # Split the calculation in batches\n",
    "        x_tilde = []\n",
    "        for i in range(img.shape[0]):\n",
    "            x_tilde.append(vq_model.decode(z[i*n:(i+1)*n]))\n",
    "        x_tilde = torch.cat(x_tilde)\n",
    "        \n",
    "        \n",
    "    return x_tilde.reshape(img.shape[0]*img.shape[1],n,*img.shape[-2:]), samples.reshape(img.shape[0],n,*code_size)\n",
    "\n",
    "reconstructions = []\n",
    "\n",
    "# for X,_ in loaded_dataloader:\n",
    "#     x_tilde, latent_sample = reconstruct(n=15,img=X, threshold_log_p=5)\n",
    "#     reconstructions.append(x_tilde)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 images experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(iter(pixel_score_loader))[0]\n",
    "reconstructionmax = reconstruct(n=5,img=X, threshold_log_p=None)[0]\n",
    "reconstruction8 = reconstruct(n=5,img=X, threshold_log_p=8)[0]\n",
    "reconstruction9 = reconstruct(n=5,img=X, threshold_log_p=9)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffrec = torch.abs(X[0] - torch.mean(reconstructionmax, dim=1))\n",
    "diff8 = torch.mean(torch.abs(reconstructionmax - reconstruction8), dim=1)\n",
    "diff9 = torch.mean(torch.abs(reconstructionmax - reconstruction9), dim=1)\n",
    "reconstructionmax = torch.mean(reconstructionmax, dim=1)\n",
    "reconstruction8 = torch.mean(reconstruction8, dim=1)\n",
    "reconstruction9 = torch.mean(reconstruction9, dim=1)\n",
    "\n",
    "fig, axes = plt.subplots(2,4, figsize=(20,10))\n",
    "\n",
    "axes[0,0].imshow(X[0].permute(1,2,0))\n",
    "axes[0,0].set_title('Original')\n",
    "axes[0,1].imshow(reconstructionmax.squeeze().permute(1,2,0))\n",
    "axes[0,1].set_title('No resampling')\n",
    "axes[0,2].imshow(reconstruction8.squeeze().permute(1,2,0))\n",
    "axes[0,2].set_title('Threshold 8')\n",
    "axes[0,3].imshow(reconstruction9.squeeze().permute(1,2,0))\n",
    "axes[0,3].set_title('Threshold 9')\n",
    "axes[1,1].imshow(diffrec.squeeze().permute(1,2,0))\n",
    "axes[1,1].set_title('|original - reconstruction|')\n",
    "axes[1,2].imshow(diff8.squeeze().permute(1,2,0))\n",
    "axes[1,2].set_title('|no resampling - threshold 8|')\n",
    "axes[1,3].imshow(diff9.squeeze().permute(1,2,0))\n",
    "axes[1,3].set_title('|no resampling - threshold 9|')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
