{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "grauvAgEvJ6i"
      },
      "source": [
        "# Anomaly detection in faces using VQ-VAE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WD4RFlV8vUW9"
      },
      "source": [
        "This notebook is in addition to the blogpost a tool to guide you through the research we performed inspired on the work of Marimont et al. on Anomaly detection through latent space restoration using VQ-VAE [1]. The purpose of this notebook is not to explain all the theoretical components of the research, rather it is to provide a step-by-step guide to obtain results on a toy data set in the form of snippets of code. This is done with the intention to deliver a greater understanding of the topic covered and to give a broad idea of the pipeline.\n",
        "The notebook can be seen as a demo for FFHQ and for FaceForensics++. We show both the same visualisations but on the two different datasets. You can play with threshold values and see what happens. This will be made clear once relevant."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE: the imports cell block is going to ask you to authorise yourself for the use of WANDB. Click the link provided in the output and copy the code on the page. Use this code to authorise and it will say:\n",
        "\n",
        "```wandb: Currently logged in as: \"user\" (vqvaeanomaly). Use `wandb login --relogin` to force relogin```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import ImageFolder\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import nets_LV\n",
        "import nets_AR\n",
        "import utils\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fake and real faces demo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will show a DEMO of the fake and real face dataset, which consists of real FFHQ faces and photoshopped ones. First, we go over the preparation steps then we give you some predictions of the model and reconstructions. Finally, a localisation method of resampled samples will be shown, highlighting the areas of the image where to model thinks an anomaly occured."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jr9G3ADC43pR"
      },
      "source": [
        "## Initialize models with pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load the checkpoints of the VQ-VAE and the PixelSnail.\n",
        "vqvae_checkpoint_path = '../AnomalyDetection/src/DeepFake/checkpoints/ffhq/vqvae.pt'\n",
        "ar_checkpoint_path = '../AnomalyDetection/src/DeepFake/checkpoints/ffhq/ar.pt'\n",
        "\n",
        "vqvae_checkpoint = torch.load(vqvae_checkpoint_path, map_location=device)\n",
        "ar_checkpoint = torch.load(ar_checkpoint_path, map_location=device)\n",
        "\n",
        "#Define the models\n",
        "vq_model = nets_LV.VQVAE(\n",
        "    d=3,\n",
        "    n_channels=(16, 32, 64, 256),\n",
        "    code_size=128,\n",
        "    n_res_block=2,\n",
        "    dropout_p=.1\n",
        ").to(device)\n",
        "\n",
        "vq_model.load_state_dict(vqvae_checkpoint[\"model\"])\n",
        "vq_model = vq_model.to(device)\n",
        "\n",
        "ar_model = nets_LV.VQLatentSNAIL(\n",
        "    feature_extractor_model=vq_model,\n",
        "    shape=(16, 16),\n",
        "    n_block=4,\n",
        "    n_res_block=4,\n",
        "    n_channels=128\n",
        ").to(device)\n",
        "\n",
        "ar_model.load_state_dict(ar_checkpoint['model'])\n",
        "ar_model = ar_model.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kgkwdUmm1kKC"
      },
      "source": [
        "## FFHQ 512 & Real/Fake faces dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This is during training the variable we use to insert augmentations, for now we will just make a tensor out of the images.\n",
        "transform_pipeline = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "#Path to toy dataset\n",
        "demo_data_path_ffhq = 'demo/demo_data/FFhq_toy'\n",
        "\n",
        "#Create the image class and dataloader.\n",
        "demo_dataset = ImageFolder(demo_data_path_ffhq, transform=transform_pipeline)\n",
        "demo_dataloader = DataLoader(demo_dataset, batch_size=3, shuffle=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample-wise score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feel free to change the 'threshold' values to see how the results will change.\n",
        "\n",
        "Since we use 'loss' to understand whether the sample is anomalous, a bigger loss == higher probablity that there is an anomaly in the image. That implies that a higher score value means that there exists an anomaly on the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Change this to adjust when the model labels the image as anomalous.\n",
        "threshold = 7\n",
        "\n",
        "pred = []\n",
        "\n",
        "ar_model.eval()\n",
        "for X, _ in tqdm(demo_dataloader):\n",
        "    with torch.no_grad():\n",
        "        X=X.to(device)\n",
        "        loss = ar_model.loss(X, reduction='none')['loss'].flatten(1)\n",
        "        \n",
        "        score = torch.sum(loss*(loss>threshold), 1).float()\n",
        "        pred.append(score.cpu().numpy())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the images with the corresponding sample-wise score. The lower the sample-wise score the less likely the image is anomalous according to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = next(iter(demo_dataloader))[0]\n",
        "fix, axes = plt.subplots(1,3, figsize=(20,10))\n",
        "\n",
        "axes[0].imshow(X[0].permute(1,2,0))\n",
        "axes[0].set_title('Sample-wise score '+ str(pred[0][0]) + \" (fake)\", fontsize=20)\n",
        "axes[1].imshow(X[1].permute(1,2,0))\n",
        "axes[1].set_title('Sample-wise score '+ str(pred[0][1]) + \" (fake)\",fontsize=20)\n",
        "axes[2].imshow(X[2].permute(1,2,0))\n",
        "axes[2].set_title('Sample-wise score '+ str(pred[0][2]) + \" (real)\",fontsize=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the visualisation above, the first 2 images are photoshopped (first is from easy category and the second from hard). The third image is a real face. The first image is modified around the eyes and nose area, the second only the node is modified. The model's prediction nicely aligns with the ground truth."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reconstructing images"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following codeblock is the function we use to show reconstructions of the models after resampling according to given threshold. The user can play with this threshold and see the effect it has on the reconstructions. The lower the threshold the more get resampled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reconstruct(n, img, threshold_log_p = 5):\n",
        "    \"\"\" Generates n reconstructions for each image in img.\n",
        "    Resamples latent variables with cross-entropy > threshold\n",
        "    Returns corrected images and associated latent variables\"\"\"\n",
        "    #Use VQ-VAE to encode original image\n",
        "    codes = ar_model.retrieve_codes(img)\n",
        "    \n",
        "    code_size = codes.shape[-2:]\n",
        "   \n",
        "    with torch.no_grad():\n",
        "        samples = codes.clone().unsqueeze(1).repeat(1,n,1,1).reshape(img.shape[0]*n,*code_size)\n",
        "    \n",
        "        if not threshold_log_p == None:\n",
        "            #The main for loop to resample latent code\n",
        "            for r in tqdm(range(code_size[0])):\n",
        "                for c in range(code_size[1]):        \n",
        "\n",
        "                    code_logits = ar_model.forward_latent(samples)[:,:,r,c]\n",
        "                    loss = F.cross_entropy(code_logits, samples[:, r, c], reduction='none')\n",
        "                    probs = F.softmax(code_logits, dim=1)\n",
        "                    #Only resample the the indices that have higher loss than the threshold\n",
        "                    samples[loss > threshold_log_p, r, c] = torch.multinomial(probs, 1).squeeze(-1)[loss > threshold_log_p]\n",
        "\n",
        "        z = vq_model.codebook.embedding(samples.unsqueeze(1))\n",
        "        z = z.squeeze(1).permute(0,3,1,2).contiguous()\n",
        "        \n",
        "\n",
        "        x_tilde = []\n",
        "        #For loop for decoding the newly samples latent codes.\n",
        "        for i in range(img.shape[0]):\n",
        "            predict = vq_model.decode(z[i*n:(i+1)*n])\n",
        "            x_tilde.append(predict)\n",
        "\n",
        "        x_tilde = torch.cat(x_tilde)\n",
        "\n",
        "    return x_tilde.reshape(img.shape[0], n, img.shape[1],*img.shape[-2:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Adjust this threshold to define how sensitive the model is to resampling latent codes.\n",
        "threshold_log=8\n",
        "\n",
        "reconstructions = []\n",
        "for X,y in demo_dataloader:\n",
        "    X = torch.Tensor(X).to(device)\n",
        "    #We create 15 reconstructions per image and average across them to get low variance results.\n",
        "    out = reconstruct(n=15,img=X.float(), threshold_log_p=threshold_log)\n",
        "    out = torch.mean(out, dim=1)\n",
        "    reconstructions.append(out)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,3, figsize=(20,10))\n",
        "fig.suptitle('Reconstructed images', fontsize=64)\n",
        "\n",
        "axes[0].imshow(reconstructions[0][0].squeeze().permute(1,2,0).cpu())\n",
        "axes[1].imshow(reconstructions[0][1].squeeze().permute(1,2,0).cpu())\n",
        "axes[2].imshow(reconstructions[0][2].squeeze().permute(1,2,0).cpu())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Differences between inputs and outputs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following codeblocks are about showing the pixels that are different in the original image compared to the reconstructions. We can show this by subtracting the reconstructed image from the original images and show the \"heatmap\" that results from this operation. This is a clear visualisation of the locations where the model finds anomalies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_diffs(input, tilde):\n",
        "    diff = torch.abs(input.to(device).float()-tilde)\n",
        "\n",
        "    # Calculate restoration weight based on restoration similarity to original image\n",
        "    sim_imgwise = torch.mean(diff,(1,2)).unsqueeze(1).unsqueeze(2)\n",
        "    sim_imgwise = torch.softmax(sim_imgwise,0)\n",
        "\n",
        "    # Pixel-wise score after weight \n",
        "    diff = (diff*sim_imgwise).sum(0,keepdims=True)\n",
        "    return diff\n",
        "\n",
        "old_woman_diff = calc_diffs(X[0], reconstructions[0][0]).permute(1,2,0).cpu()\n",
        "young_woman_diff = calc_diffs(X[1], reconstructions[0][1]).permute(1,2,0).cpu()\n",
        "man_diff = calc_diffs(X[2], reconstructions[0][2]).permute(1,2,0).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix, axes = plt.subplots(1,3, figsize=(20,10))\n",
        "fix.suptitle('Difference reconstructed vs original', fontsize=64)\n",
        "axes[0].imshow(old_woman_diff)\n",
        "axes[0].set_title('')\n",
        "axes[1].imshow(young_woman_diff)\n",
        "axes[1].set_title('')\n",
        "axes[2].imshow(man_diff)\n",
        "axes[2].set_title('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the rightmost face is a real one, the two before are modified. Some interesting obervations can be made. For the first face we see that resampling happened more on the top of the head. The accessories of the woman's clothing were detected as outliers and the main reason for higher sample-wise score. For the middle face we see that the eyes were detected as anomalous, and we see some parts of the nose-ring light up. The reason for the eyes could be that the woman is looking away from the camera and at a slight angle, which is different from the prior learnt. Finally the last real face is mostly what should be expected, low activity."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FaceForensics ++ demo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we show similar steps but for the face forensics++ dataset. NOTE: that faceforensics is only accessible after signing up for it. For the purposes of this DEMO we provide you with a toy set of preprocessed faceforensics++ images."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize models with pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vqvae_checkpoint_path = '../AnomalyDetection/src/DeepFake/checkpoints/faceforensics/vqvae.pt'\n",
        "vqvae_checkpoint = torch.load(vqvae_checkpoint_path, map_location=device)\n",
        "\n",
        "ar_checkpoint_path = '../AnomalyDetection/src/DeepFake/demo/checkpoints/faceforensics/ar.pt'\n",
        "ar_checkpoint = torch.load(ar_checkpoint_path, map_location=device)\n",
        "\n",
        "vq_model = nets_LV.VQVAE(\n",
        "    d=3,\n",
        "    n_channels=(16, 32, 64, 256),\n",
        "    code_size=128,\n",
        "    n_res_block=2,\n",
        "    dropout_p=.1\n",
        ").to(device)\n",
        "\n",
        "vq_model.load_state_dict(vqvae_checkpoint[\"model\"])\n",
        "vq_model = vq_model.to(device)\n",
        "\n",
        "ar_model = nets_LV.VQLatentSNAIL(\n",
        "    feature_extractor_model=vq_model,\n",
        "    shape=(16, 16),\n",
        "    n_block=4,\n",
        "    n_res_block=4,\n",
        "    n_channels=128\n",
        ").to(device)\n",
        "\n",
        "ar_model.load_state_dict(ar_checkpoint['model'])\n",
        "ar_model = ar_model.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QU4cfJoX1pf8"
      },
      "source": [
        "## FaceForensics++ data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform_pipeline = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "demo_data_path_forensic = '../AnomalyDetection/src/DeepFake/demo/demo_data/forensic_toy'\n",
        "\n",
        "demo_dataset = ImageFolder(demo_data_path_forensic, transform=transform_pipeline)\n",
        "demo_dataloader = DataLoader(demo_dataset, batch_size=3, shuffle=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample-wise score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold = 7\n",
        "\n",
        "pred = []\n",
        "\n",
        "ar_model.eval()\n",
        "for X, _ in tqdm(demo_dataloader):\n",
        "    with torch.no_grad():\n",
        "        X=X.to(device)\n",
        "        loss = ar_model.loss(X, reduction='none')['loss'].flatten(1)\n",
        "        \n",
        "        score = torch.sum(loss*(loss>threshold), 1).float()\n",
        "        pred.append(score.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = next(iter(demo_dataloader))[0]\n",
        "fix, axes = plt.subplots(1,3, figsize=(20,10))\n",
        "axes[0].imshow(X[0].permute(1,2,0))\n",
        "axes[0].set_title('Sample-wise score '+ str(pred[0][0]), fontsize=20)\n",
        "axes[1].imshow(X[1].permute(1,2,0))\n",
        "axes[1].set_title('Sample-wise score '+ str(pred[0][1]), fontsize=20)\n",
        "axes[2].imshow(X[2].permute(1,2,0))\n",
        "axes[2].set_title('Sample-wise score '+ str(pred[0][2]), fontsize=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FaceForensics reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reconstruct(n, img, threshold_log_p = 5):\n",
        "    \"\"\" Generates n reconstructions for each image in img.\n",
        "    Resamples latent variables with cross-entropy > threshold\n",
        "    Returns corrected images and associated latent variables\"\"\"\n",
        "    #Use VQ-VAE to encode original image\n",
        "    codes = ar_model.retrieve_codes(img)\n",
        "    \n",
        "    code_size = codes.shape[-2:]\n",
        "   \n",
        "    with torch.no_grad():\n",
        "        samples = codes.clone().unsqueeze(1).repeat(1,n,1,1).reshape(img.shape[0]*n,*code_size)\n",
        "    \n",
        "        if not threshold_log_p == None:\n",
        "            #The main for loop to resample latent code\n",
        "            for r in tqdm(range(code_size[0])):\n",
        "                for c in range(code_size[1]):        \n",
        "\n",
        "                    code_logits = ar_model.forward_latent(samples)[:,:,r,c]\n",
        "                    loss = F.cross_entropy(code_logits, samples[:, r, c], reduction='none')\n",
        "                    probs = F.softmax(code_logits, dim=1)\n",
        "                    #Only resample the the indices that have higher loss than the threshold\n",
        "                    samples[loss > threshold_log_p, r, c] = torch.multinomial(probs, 1).squeeze(-1)[loss > threshold_log_p]\n",
        "\n",
        "        z = vq_model.codebook.embedding(samples.unsqueeze(1))\n",
        "        z = z.squeeze(1).permute(0,3,1,2).contiguous()\n",
        "        \n",
        "\n",
        "        x_tilde = []\n",
        "        #For loop for decoding the newly samples latent codes.\n",
        "        for i in range(img.shape[0]):\n",
        "            predict = vq_model.decode(z[i*n:(i+1)*n])\n",
        "            x_tilde.append(predict)\n",
        "\n",
        "        x_tilde = torch.cat(x_tilde)\n",
        "\n",
        "    return x_tilde.reshape(img.shape[0], n, img.shape[1],*img.shape[-2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reconstructions = []\n",
        "for X,y in demo_dataloader:\n",
        "    X = torch.Tensor(X).to(device)\n",
        "    out = reconstruct(n=15,img=X.float(), threshold_log_p=8)\n",
        "    out = torch.mean(out, dim=1)\n",
        "    reconstructions.append(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,3, figsize=(20,10))\n",
        "fig.suptitle('Reconstructed images', fontsize=64)\n",
        "\n",
        "axes[0].imshow(reconstructions[0][0].squeeze().permute(1,2,0).cpu())\n",
        "axes[1].imshow(reconstructions[0][1].squeeze().permute(1,2,0).cpu())\n",
        "axes[2].imshow(reconstructions[0][2].squeeze().permute(1,2,0).cpu())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Differences between inputs and outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_diffs(input, tilde):\n",
        "    diff = torch.abs(input.to(device).float()-tilde)\n",
        "\n",
        "    # Calculate restoration weight based on restoration similarity to original image\n",
        "    sim_imgwise = torch.mean(diff,(1,2)).unsqueeze(1).unsqueeze(2)\n",
        "    sim_imgwise = torch.softmax(sim_imgwise,0)\n",
        "\n",
        "    # Pixel-wise score after weight \n",
        "    diff = (diff*sim_imgwise).sum(0,keepdims=True)\n",
        "\n",
        "    return diff\n",
        "\n",
        "old_woman_diff = calc_diffs(X[0], reconstructions[0][0]).permute(1,2,0).cpu()\n",
        "young_woman_diff = calc_diffs(X[1], reconstructions[0][1]).permute(1,2,0).cpu()\n",
        "man_diff = calc_diffs(X[2], reconstructions[0][2]).permute(1,2,0).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fix, axes = plt.subplots(1,3, figsize=(20,10))\n",
        "fix.suptitle('Difference reconstructed vs original', fontsize=64)\n",
        "axes[0].imshow(old_woman_diff)\n",
        "axes[0].set_title('')\n",
        "axes[1].imshow(young_woman_diff)\n",
        "axes[1].set_title('')\n",
        "axes[2].imshow(man_diff)\n",
        "axes[2].set_title('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the first face we can see how some parts of the eye and left eyebrow are being resampled, and actually being fixed in the restoration process. The face in the middle is a real face so we expect low resampling activity. Finally the 3rd face we can see nicely how the model finds the artefacts as a result of modifications and resamples those cases."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
