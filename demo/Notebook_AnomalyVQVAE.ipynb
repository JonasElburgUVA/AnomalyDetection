{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "grauvAgEvJ6i"
      },
      "source": [
        "# Anomaly detection in faces using VQ-VAE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Izwifj64Lo5J"
      },
      "source": [
        "**Show necessary code blocks to reproduce some examples. Some reconstructions, evalutation scores etc.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WD4RFlV8vUW9"
      },
      "source": [
        "This notebook is in addition to the blogpost a tool to guide you through the research we performed inspired on the work of Marimont et al. on Anomaly detection through latent space restoration using VQ-VAE [1]. The purpose of this notebook is not to explain all the theoretical components of the research, rather it is to provide a step-by-step guide to obtain results on a toy data set in the form of snippets of code in conjunction with explanatory text. This is done with the intention to deliver a greater understanding of the topic covered.\n",
        "The structure of this notebook is as follows:\n",
        "1.   go over the necessary pre requisites needed to understand the project. \n",
        "2.   Delve deeper into the components of the original work showing snippets of code that are relevant.\n",
        "3. Introduce the setup of the faces datasets.\n",
        "4. Briefly go over pre-processing changes and relevant code.\n",
        "5. Highlight the changes that are necessary to adapt the original models to fit the new context.\n",
        "6. Touch upon the metrics used for evaluating and showing a summary of quantitative and qualitative results.\n",
        "\n",
        "**Abstract**\n",
        "(...)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jNxjDYa4x_22"
      },
      "source": [
        "# 1. Pre-requisites\n",
        "\n",
        "In this section we are going to present some of the fundamental components to have a better understanding of the proposed methodology. This means we will cover the VQ-VAE and the PixelSnail. Additionally, we will cover the structure of the data, this will be necessary to understand the changes made later.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import ImageFolder\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import utils.utils as utils\n",
        "import utils.nets_LV as nets_LV"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fake and real faces demo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jr9G3ADC43pR"
      },
      "source": [
        "## Initialize models with pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "vqvae_checkpoint_path = 'demo_checkpoints/ffhq_vqvae.pt'\n",
        "ar_checkpoint_path = 'demo_checkpoints/ffhq_ar.pt'\n",
        "ar_checkpoint = torch.load(ar_checkpoint_path, map_location=device)\n",
        "vqvae_checkpoint = torch.load(vqvae_checkpoint_path, map_location=device)\n",
        "\n",
        "vq_model = nets_LV.VQVAE(\n",
        "    d=3,\n",
        "    n_channels=(16, 32, 64, 256),\n",
        "    code_size=128,\n",
        "    n_res_block=2,\n",
        "    dropout_p=.1\n",
        ").to(device)\n",
        "\n",
        "vq_model.load_state_dict(vqvae_checkpoint[\"model\"])\n",
        "vq_model = vq_model.to(device)\n",
        "\n",
        "ar_model = nets_LV.VQLatentSNAIL(\n",
        "    feature_extractor_model=vq_model,\n",
        "    shape=(16, 16),\n",
        "    n_block=4,\n",
        "    n_res_block=4,\n",
        "    n_channels=128\n",
        ").to(device)\n",
        "\n",
        "ar_model.load_state_dict(ar_checkpoint['model'])\n",
        "ar_model = ar_model.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kgkwdUmm1kKC"
      },
      "source": [
        "## FFHQ 512 & Real/Fake faces dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform_pipeline = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "demo_data_path_ffhq = 'demo_data/fakeFF_toy/'\n",
        "\n",
        "\n",
        "demo_dataset = ImageFolder(demo_data_path_ffhq, transform=transform_pipeline)\n",
        "demo_dataloader = DataLoader(demo_dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample wise score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold = 7\n",
        "\n",
        "pred = []\n",
        "\n",
        "ar_model.eval()\n",
        "for X, _ in tqdm(demo_dataloader):\n",
        "    x_img = torch.from_numpy(X.img.copy()).to(device).float()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        codes = ar_model.retrieve_codes(x_img).flatten(1)\n",
        "        loss = ar_model.loss(X, reduction='none')['loss'].flatten(1)\n",
        "        \n",
        "        score = torch.sum(loss*(loss>threshold), 1).float()\n",
        "        pred.append([score.cpu().numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "no = 10\n",
        "fig,ax = plt.subplots(1,4,figsize = (3,3))\n",
        "\n",
        "[a.imshow(next(iter(demo_dataloader))[0][i].img[no],vmin=-3,vmax=3) for i,a in enumerate(ax)]\n",
        "[a.set_title('Score {:.1f}'.format(pred[i])) for i,a in enumerate(ax)]\n",
        "[a.axis('off') for a in ax]\n",
        "plt.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reconstructing with pixel-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reconstruct(n, img, threshold_log_p = 5):\n",
        "    \"\"\" Generates n reconstructions for each image in img.\n",
        "    Resamples latent variables with cross-entropy > threshold\n",
        "    Returns corrected images and associated latent variables\"\"\"\n",
        "          \n",
        "    #Use VQ-VAE to encode original image\n",
        "    codes = ar_model.retrieve_codes(img)\n",
        "    code_size = codes.shape[-2:]\n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "        samples = codes.clone().unsqueeze(1).repeat(1,n,1,1).reshape(img.shape[0]*n,*code_size)\n",
        "\n",
        "        if not threshold_log_p == None:\n",
        "            for r in tqdm(range(code_size[0])):\n",
        "                for c in range(code_size[1]):        \n",
        "\n",
        "                    code_logits = ar_model.forward_latent(samples)[:,:,r,c]\n",
        "                    loss = F.cross_entropy(code_logits, samples[:, r, c], reduction='none')\n",
        "                    probs = F.softmax(code_logits, dim=1)\n",
        "\n",
        "                    samples[loss > threshold_log_p, r, c] = torch.multinomial(probs, 1).squeeze(-1)[loss > threshold_log_p]\n",
        "\n",
        "        z = vq_model.codebook.embedding(samples.unsqueeze(1))\n",
        "        z = z.squeeze(1).permute(0,3,1,2).contiguous()\n",
        "        \n",
        "        # Split the calculation in batches\n",
        "        x_tilde = []\n",
        "        for i in range(img.shape[0]):\n",
        "            x_tilde.append(vq_model.decode(z[i*n:(i+1)*n]))\n",
        "        x_tilde = torch.cat(x_tilde)\n",
        "        \n",
        "        \n",
        "    return x_tilde.reshape(img.shape[0]*img.shape[1],n,*img.shape[-2:]), samples.reshape(img.shape[0],n,*code_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reconstructions = []\n",
        "for X in next(iter(demo_dataloader)):\n",
        "    reconstructions.append(reconstruct(n=5,img=X, threshold_log_p=8)[0])\n",
        "    \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FaceForensics ++ demo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize models with pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vqvae_checkpoint_path = 'demo_checkpoints/faceforensics_vqvae.pt'\n",
        "vqvae_checkpoint = torch.load(vqvae_checkpoint_path, map_location=device)\n",
        "\n",
        "ar_checkpoint_path = 'demo_checkpoints/faceforensics_ar.pt'\n",
        "ar_checkpoint = torch.load(ar_checkpoint_path, map_location=device)\n",
        "\n",
        "vq_model = nets_LV.VQVAE(\n",
        "    d=3,\n",
        "    n_channels=(16, 32, 64, 256),\n",
        "    code_size=128,\n",
        "    n_res_block=2,\n",
        "    dropout_p=.1\n",
        ").to(device)\n",
        "\n",
        "vq_model.load_state_dict(vqvae_checkpoint[\"model\"])\n",
        "vq_model = vq_model.to(device)\n",
        "\n",
        "ar_model = nets_LV.VQLatentSNAIL(\n",
        "    feature_extractor_model=vq_model,\n",
        "    shape=(16, 16),\n",
        "    n_block=4,\n",
        "    n_res_block=4,\n",
        "    n_channels=128\n",
        ").to(device)\n",
        "\n",
        "ar_model.load_state_dict(ar_checkpoint['model'])\n",
        "ar_model = ar_model.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QU4cfJoX1pf8"
      },
      "source": [
        "## FaceForensics++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform_pipeline = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "demo_data_path_ffhq = 'demo_data/faceforensics'\n",
        "\n",
        "demo_dataset = ImageFolder(demo_data_path_ffhq, transform=transform_pipeline)\n",
        "demo_dataloader = DataLoader(demo_dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_YBGt9FuyPZt"
      },
      "source": [
        "# 4. Pre-processing pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AZLwlpUWyyFW"
      },
      "source": [
        "# 5. Modifications original methodology"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R0YDbLIqy9jp"
      },
      "source": [
        "# 6. Evaluation and discussion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0Zxk2XYx_RK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJe-9TbavBdR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
